{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "h6_zRLmbPz5A"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"water.csv\")\n",
    "dlcols = ['Hardness','Chloramines','Sulfate','Organic_carbon','Trihalomethanes']\n",
    "for column in dlcols:\n",
    "    del df[column]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RkemyH3DQslg",
    "outputId": "c28d948b-fea9-43a5-fa67-c13d05bf8a43"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ph              0\n",
       "Solids          0\n",
       "Conductivity    0\n",
       "Turbidity       0\n",
       "Potability      0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.dropna()\n",
    "df.isna().sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "GqTyrITJTDCl",
    "outputId": "c51750b0-02e9-4f03-8534-54259fee1ff9"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <div id=\"df-95347323-cdec-4faf-ae98-73e414e87873\" class=\"colab-df-container\">\n",
       "    <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ph</th>\n",
       "      <th>Solids</th>\n",
       "      <th>Conductivity</th>\n",
       "      <th>Turbidity</th>\n",
       "      <th>Potability</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3.716080</td>\n",
       "      <td>18630.057858</td>\n",
       "      <td>592.885359</td>\n",
       "      <td>4.500656</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8.099124</td>\n",
       "      <td>19909.541732</td>\n",
       "      <td>418.606213</td>\n",
       "      <td>3.055934</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8.316766</td>\n",
       "      <td>22018.417441</td>\n",
       "      <td>363.266516</td>\n",
       "      <td>4.628771</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9.092223</td>\n",
       "      <td>17978.986339</td>\n",
       "      <td>398.410813</td>\n",
       "      <td>4.075075</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5.584087</td>\n",
       "      <td>28748.687739</td>\n",
       "      <td>280.467916</td>\n",
       "      <td>2.559708</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "    <div class=\"colab-df-buttons\">\n",
       "\n",
       "  <div class=\"colab-df-container\">\n",
       "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-95347323-cdec-4faf-ae98-73e414e87873')\"\n",
       "            title=\"Convert this dataframe to an interactive table.\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
       "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
       "  </svg>\n",
       "    </button>\n",
       "\n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    .colab-df-buttons div {\n",
       "      margin-bottom: 4px;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "    <script>\n",
       "      const buttonEl =\n",
       "        document.querySelector('#df-95347323-cdec-4faf-ae98-73e414e87873 button.colab-df-convert');\n",
       "      buttonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "      async function convertToInteractive(key) {\n",
       "        const element = document.querySelector('#df-95347323-cdec-4faf-ae98-73e414e87873');\n",
       "        const dataTable =\n",
       "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                    [key], {});\n",
       "        if (!dataTable) return;\n",
       "\n",
       "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "          + ' to learn more about interactive tables.';\n",
       "        element.innerHTML = '';\n",
       "        dataTable['output_type'] = 'display_data';\n",
       "        await google.colab.output.renderOutput(dataTable, element);\n",
       "        const docLink = document.createElement('div');\n",
       "        docLink.innerHTML = docLinkHtml;\n",
       "        element.appendChild(docLink);\n",
       "      }\n",
       "    </script>\n",
       "  </div>\n",
       "\n",
       "\n",
       "<div id=\"df-b8586b78-618f-48ba-9d1e-ee67ee982e6d\">\n",
       "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-b8586b78-618f-48ba-9d1e-ee67ee982e6d')\"\n",
       "            title=\"Suggest charts.\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "     width=\"24px\">\n",
       "    <g>\n",
       "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
       "    </g>\n",
       "</svg>\n",
       "  </button>\n",
       "\n",
       "<style>\n",
       "  .colab-df-quickchart {\n",
       "      --bg-color: #E8F0FE;\n",
       "      --fill-color: #1967D2;\n",
       "      --hover-bg-color: #E2EBFA;\n",
       "      --hover-fill-color: #174EA6;\n",
       "      --disabled-fill-color: #AAA;\n",
       "      --disabled-bg-color: #DDD;\n",
       "  }\n",
       "\n",
       "  [theme=dark] .colab-df-quickchart {\n",
       "      --bg-color: #3B4455;\n",
       "      --fill-color: #D2E3FC;\n",
       "      --hover-bg-color: #434B5C;\n",
       "      --hover-fill-color: #FFFFFF;\n",
       "      --disabled-bg-color: #3B4455;\n",
       "      --disabled-fill-color: #666;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart {\n",
       "    background-color: var(--bg-color);\n",
       "    border: none;\n",
       "    border-radius: 50%;\n",
       "    cursor: pointer;\n",
       "    display: none;\n",
       "    fill: var(--fill-color);\n",
       "    height: 32px;\n",
       "    padding: 0;\n",
       "    width: 32px;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart:hover {\n",
       "    background-color: var(--hover-bg-color);\n",
       "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "    fill: var(--button-hover-fill-color);\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart-complete:disabled,\n",
       "  .colab-df-quickchart-complete:disabled:hover {\n",
       "    background-color: var(--disabled-bg-color);\n",
       "    fill: var(--disabled-fill-color);\n",
       "    box-shadow: none;\n",
       "  }\n",
       "\n",
       "  .colab-df-spinner {\n",
       "    border: 2px solid var(--fill-color);\n",
       "    border-color: transparent;\n",
       "    border-bottom-color: var(--fill-color);\n",
       "    animation:\n",
       "      spin 1s steps(1) infinite;\n",
       "  }\n",
       "\n",
       "  @keyframes spin {\n",
       "    0% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "      border-left-color: var(--fill-color);\n",
       "    }\n",
       "    20% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    30% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    40% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    60% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    80% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "    90% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "  }\n",
       "</style>\n",
       "\n",
       "  <script>\n",
       "    async function quickchart(key) {\n",
       "      const quickchartButtonEl =\n",
       "        document.querySelector('#' + key + ' button');\n",
       "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
       "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
       "      try {\n",
       "        const charts = await google.colab.kernel.invokeFunction(\n",
       "            'suggestCharts', [key], {});\n",
       "      } catch (error) {\n",
       "        console.error('Error during call to suggestCharts:', error);\n",
       "      }\n",
       "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
       "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
       "    }\n",
       "    (() => {\n",
       "      let quickchartButtonEl =\n",
       "        document.querySelector('#df-b8586b78-618f-48ba-9d1e-ee67ee982e6d button');\n",
       "      quickchartButtonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "    })();\n",
       "  </script>\n",
       "</div>\n",
       "    </div>\n",
       "  </div>\n"
      ],
      "text/plain": [
       "         ph        Solids  Conductivity  Turbidity  Potability\n",
       "1  3.716080  18630.057858    592.885359   4.500656           0\n",
       "2  8.099124  19909.541732    418.606213   3.055934           0\n",
       "3  8.316766  22018.417441    363.266516   4.628771           0\n",
       "4  9.092223  17978.986339    398.410813   4.075075           0\n",
       "5  5.584087  28748.687739    280.467916   2.559708           0"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MHuZ-d_hRLnF",
    "outputId": "fbfaaba2-38e0-45fc-ffc0-1d0c52db0bcb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 2785 entries, 1 to 3275\n",
      "Data columns (total 5 columns):\n",
      " #   Column        Non-Null Count  Dtype  \n",
      "---  ------        --------------  -----  \n",
      " 0   ph            2785 non-null   float64\n",
      " 1   Solids        2785 non-null   float64\n",
      " 2   Conductivity  2785 non-null   float64\n",
      " 3   Turbidity     2785 non-null   float64\n",
      " 4   Potability    2785 non-null   int64  \n",
      "dtypes: float64(4), int64(1)\n",
      "memory usage: 130.5 KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "p1g1RbeQROPY",
    "outputId": "a4900713-e94b-410e-f4bb-5bc1b4d3fcc2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.5978456014362658\n",
      "Confusion Matrix:\n",
      " [[333   0]\n",
      " [224   0]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      1.00      0.75       333\n",
      "           1       0.00      0.00      0.00       224\n",
      "\n",
      "    accuracy                           0.60       557\n",
      "   macro avg       0.30      0.50      0.37       557\n",
      "weighted avg       0.36      0.60      0.45       557\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "X = df.drop('Potability', axis=1)\n",
    "y = df['Potability']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "model = LogisticRegression(random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "classification_rep = classification_report(y_test, y_pred)\n",
    "\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Confusion Matrix:\\n\", conf_matrix)\n",
    "print(\"Classification Report:\\n\", classification_rep)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "D_soUTuVR91X",
    "outputId": "71f9682c-c0fd-4c1f-ab10-a9fd259ae038"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Accuracy: 0.5978456014362658\n",
      "SVM Accuracy: 0.5978456014362658\n",
      "Decision Tree Accuracy: 0.5134649910233393\n",
      "Random Forest Accuracy: 0.5529622980251346\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "X = df.drop('Potability', axis=1)\n",
    "y = df['Potability']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Logistic Regression\n",
    "model_lr = LogisticRegression(random_state=42)\n",
    "model_lr.fit(X_train, y_train)\n",
    "y_pred_lr = model_lr.predict(X_test)\n",
    "accuracy_lr = accuracy_score(y_test, y_pred_lr)\n",
    "print(\"Logistic Regression Accuracy:\", accuracy_lr)\n",
    "\n",
    "# Support Vector Machine (SVM)\n",
    "model_svm = SVC(random_state=42)\n",
    "model_svm.fit(X_train, y_train)\n",
    "y_pred_svm = model_svm.predict(X_test)\n",
    "accuracy_svm = accuracy_score(y_test, y_pred_svm)\n",
    "print(\"SVM Accuracy:\", accuracy_svm)\n",
    "\n",
    "# Decision Tree\n",
    "model_dt = DecisionTreeClassifier(random_state=42)\n",
    "model_dt.fit(X_train, y_train)\n",
    "y_pred_dt = model_dt.predict(X_test)\n",
    "accuracy_dt = accuracy_score(y_test, y_pred_dt)\n",
    "print(\"Decision Tree Accuracy:\", accuracy_dt)\n",
    "\n",
    "# Random Forest\n",
    "model_rf = RandomForestClassifier(random_state=42)\n",
    "model_rf.fit(X_train, y_train)\n",
    "y_pred_rf = model_rf.predict(X_test)\n",
    "accuracy_rf = accuracy_score(y_test, y_pred_rf)\n",
    "print(\"Random Forest Accuracy:\", accuracy_rf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1dDZRSoUSNpP",
    "outputId": "ba77c9c9-b15c-4e60-b726-04b49cd59ea2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Accuracy (with PCA): 0.5978456014362658\n",
      "SVM Accuracy (with PCA): 0.5978456014362658\n",
      "Decision Tree Accuracy (with PCA): 0.547576301615799\n",
      "Random Forest Accuracy (with PCA): 0.5601436265709157\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "X = df.drop('Potability', axis=1)\n",
    "y = df['Potability']\n",
    "\n",
    "# Split the data into a training set and a testing set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standard Scaling\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# PCA\n",
    "pca = PCA(n_components=3)  # You can adjust the number of components as needed\n",
    "X_train_pca = pca.fit_transform(X_train_scaled)\n",
    "X_test_pca = pca.transform(X_test_scaled)\n",
    "\n",
    "# Logistic Regression\n",
    "model_lr = LogisticRegression(random_state=42)\n",
    "model_lr.fit(X_train_pca, y_train)\n",
    "y_pred_lr = model_lr.predict(X_test_pca)\n",
    "accuracy_lr = accuracy_score(y_test, y_pred_lr)\n",
    "print(\"Logistic Regression Accuracy (with PCA):\", accuracy_lr)\n",
    "\n",
    "# Support Vector Machine (SVM)\n",
    "model_svm = SVC(random_state=42)\n",
    "model_svm.fit(X_train_pca, y_train)\n",
    "y_pred_svm = model_svm.predict(X_test_pca)\n",
    "accuracy_svm = accuracy_score(y_test, y_pred_svm)\n",
    "print(\"SVM Accuracy (with PCA):\", accuracy_svm)\n",
    "\n",
    "# Decision Tree\n",
    "model_dt = DecisionTreeClassifier(random_state=42)\n",
    "model_dt.fit(X_train_pca, y_train)\n",
    "y_pred_dt = model_dt.predict(X_test_pca)\n",
    "accuracy_dt = accuracy_score(y_test, y_pred_dt)\n",
    "print(\"Decision Tree Accuracy (with PCA):\", accuracy_dt)\n",
    "\n",
    "# Random Forest\n",
    "model_rf = RandomForestClassifier(random_state=42)\n",
    "model_rf.fit(X_train_pca, y_train)\n",
    "y_pred_rf = model_rf.predict(X_test_pca)\n",
    "accuracy_rf = accuracy_score(y_test, y_pred_rf)\n",
    "print(\"Random Forest Accuracy (with PCA):\", accuracy_rf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Tzre6yLHWF1V"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load your existing Excel file into a DataFrame (replace 'your_file.xlsx' with your file path)\n",
    "df = pd.read_excel('waterdf1.xlsx')\n",
    "\n",
    "# Create a list of sensor names (assuming you have 5 sensors)\n",
    "sensor_names = ['Sensor 1', 'Sensor 2', 'Sensor 3', 'Sensor 4', 'Sensor 5']\n",
    "\n",
    "# Calculate the number of times each sensor name should be repeated to distribute them equally\n",
    "num_rows = len(df)\n",
    "num_sensors = len(sensor_names)\n",
    "repeat_count = num_rows // num_sensors\n",
    "\n",
    "# Create the \"Sensor Name\" column by repeating the sensor names\n",
    "df['Sensor Name'] = sensor_names * repeat_count\n",
    "\n",
    "# If there are remaining rows, distribute the sensor names evenly among them\n",
    "if num_rows % num_sensors != 0:\n",
    "    remainder = num_rows % num_sensors\n",
    "    df['Sensor Name'] = df['Sensor Name'].append(pd.Series(sensor_names[:remainder] * repeat_count))\n",
    "\n",
    "# Save the updated DataFrame back to the Excel file\n",
    "df.to_excel('updated_file.xlsx', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "X2-0NkAeXi6s"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Load your existing Excel file into a DataFrame (replace 'your_file.xlsx' with your file path)\n",
    "df = pd.read_excel('waterdf1.xlsx')\n",
    "\n",
    "# Create a list of sensor names (assuming you have 5 sensors)\n",
    "sensor_names = ['Sensor 1', 'Sensor 2', 'Sensor 3', 'Sensor 4', 'Sensor 5']\n",
    "\n",
    "# Calculate the number of times each sensor name should be repeated to distribute them equally\n",
    "num_rows = len(df)\n",
    "num_sensors = len(sensor_names)\n",
    "repeat_count = num_rows // num_sensors\n",
    "\n",
    "# Create a timestamp column\n",
    "timestamp_list = []\n",
    "\n",
    "# Start with the current time\n",
    "current_time = datetime.now()\n",
    "\n",
    "for _ in range(repeat_count):\n",
    "    for sensor_name in sensor_names:\n",
    "        timestamp_list.append(current_time)\n",
    "    current_time += timedelta(minutes=1)\n",
    "\n",
    "# If there are remaining rows, distribute the timestamps evenly among them\n",
    "if num_rows % num_sensors != 0:\n",
    "    remainder = num_rows % num_sensors\n",
    "    current_time = datetime.now()\n",
    "    for _ in range(remainder):\n",
    "        timestamp_list.append(current_time)\n",
    "        current_time += timedelta(minutes=1)\n",
    "\n",
    "# Add the \"Sensor Name\" and \"Timestamp\" columns to the DataFrame\n",
    "df['Sensor Name'] = sensor_names * repeat_count\n",
    "df['Timestamp'] = timestamp_list\n",
    "\n",
    "# Save the updated DataFrame back to the Excel file\n",
    "df.to_excel('updated_file_with_timestamps.xlsx', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ann"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "47/47 - 0s - loss: 785.3136 - val_loss: 278.1748 - 378ms/epoch - 8ms/step\n",
      "Epoch 2/100\n",
      "47/47 - 0s - loss: 770.8782 - val_loss: 264.6179 - 56ms/epoch - 1ms/step\n",
      "Epoch 3/100\n",
      "47/47 - 0s - loss: 741.5561 - val_loss: 249.7769 - 49ms/epoch - 1ms/step\n",
      "Epoch 4/100\n",
      "47/47 - 0s - loss: 713.5439 - val_loss: 253.5554 - 48ms/epoch - 1ms/step\n",
      "Epoch 5/100\n",
      "47/47 - 0s - loss: 697.1459 - val_loss: 259.5386 - 61ms/epoch - 1ms/step\n",
      "Epoch 6/100\n",
      "47/47 - 0s - loss: 688.5174 - val_loss: 293.7461 - 53ms/epoch - 1ms/step\n",
      "Epoch 7/100\n",
      "47/47 - 0s - loss: 684.9992 - val_loss: 289.5481 - 57ms/epoch - 1ms/step\n",
      "Epoch 8/100\n",
      "47/47 - 0s - loss: 683.2989 - val_loss: 335.9575 - 60ms/epoch - 1ms/step\n",
      "Epoch 9/100\n",
      "47/47 - 0s - loss: 680.1058 - val_loss: 349.0848 - 58ms/epoch - 1ms/step\n",
      "Epoch 10/100\n",
      "47/47 - 0s - loss: 678.8192 - val_loss: 347.1672 - 59ms/epoch - 1ms/step\n",
      "Epoch 11/100\n",
      "47/47 - 0s - loss: 680.1078 - val_loss: 387.3904 - 58ms/epoch - 1ms/step\n",
      "Epoch 12/100\n",
      "47/47 - 0s - loss: 676.9411 - val_loss: 394.0193 - 59ms/epoch - 1ms/step\n",
      "Epoch 13/100\n",
      "47/47 - 0s - loss: 675.3163 - val_loss: 410.2767 - 58ms/epoch - 1ms/step\n",
      "Epoch 14/100\n",
      "47/47 - 0s - loss: 674.7335 - val_loss: 424.3138 - 53ms/epoch - 1ms/step\n",
      "Epoch 15/100\n",
      "47/47 - 0s - loss: 674.8017 - val_loss: 450.8380 - 57ms/epoch - 1ms/step\n",
      "Epoch 16/100\n",
      "47/47 - 0s - loss: 673.7764 - val_loss: 460.2327 - 59ms/epoch - 1ms/step\n",
      "Epoch 17/100\n",
      "47/47 - 0s - loss: 674.3940 - val_loss: 471.6757 - 59ms/epoch - 1ms/step\n",
      "Epoch 18/100\n",
      "47/47 - 0s - loss: 675.9019 - val_loss: 456.5977 - 47ms/epoch - 998us/step\n",
      "Epoch 19/100\n",
      "47/47 - 0s - loss: 674.1679 - val_loss: 466.1052 - 57ms/epoch - 1ms/step\n",
      "Epoch 20/100\n",
      "47/47 - 0s - loss: 672.2205 - val_loss: 421.9507 - 50ms/epoch - 1ms/step\n",
      "Epoch 21/100\n",
      "47/47 - 0s - loss: 672.3466 - val_loss: 434.8564 - 65ms/epoch - 1ms/step\n",
      "Epoch 22/100\n",
      "47/47 - 0s - loss: 671.5984 - val_loss: 451.5440 - 50ms/epoch - 1ms/step\n",
      "Epoch 23/100\n",
      "47/47 - 0s - loss: 671.8652 - val_loss: 468.9254 - 54ms/epoch - 1ms/step\n",
      "Epoch 24/100\n",
      "47/47 - 0s - loss: 671.8939 - val_loss: 501.9068 - 57ms/epoch - 1ms/step\n",
      "Epoch 25/100\n",
      "47/47 - 0s - loss: 670.5804 - val_loss: 487.5649 - 54ms/epoch - 1ms/step\n",
      "Epoch 26/100\n",
      "47/47 - 0s - loss: 670.8249 - val_loss: 506.2320 - 54ms/epoch - 1ms/step\n",
      "Epoch 27/100\n",
      "47/47 - 0s - loss: 670.0800 - val_loss: 500.8103 - 56ms/epoch - 1ms/step\n",
      "Epoch 28/100\n",
      "47/47 - 0s - loss: 668.7650 - val_loss: 515.9154 - 54ms/epoch - 1ms/step\n",
      "Epoch 29/100\n",
      "47/47 - 0s - loss: 668.9988 - val_loss: 489.6179 - 59ms/epoch - 1ms/step\n",
      "Epoch 30/100\n",
      "47/47 - 0s - loss: 668.2336 - val_loss: 518.3884 - 59ms/epoch - 1ms/step\n",
      "Epoch 31/100\n",
      "47/47 - 0s - loss: 668.3506 - val_loss: 545.8788 - 58ms/epoch - 1ms/step\n",
      "Epoch 32/100\n",
      "47/47 - 0s - loss: 667.7333 - val_loss: 570.0881 - 59ms/epoch - 1ms/step\n",
      "Epoch 33/100\n",
      "47/47 - 0s - loss: 668.6638 - val_loss: 567.3562 - 64ms/epoch - 1ms/step\n",
      "Epoch 34/100\n",
      "47/47 - 0s - loss: 667.2205 - val_loss: 582.7971 - 58ms/epoch - 1ms/step\n",
      "Epoch 35/100\n",
      "47/47 - 0s - loss: 668.2950 - val_loss: 574.9122 - 57ms/epoch - 1ms/step\n",
      "Epoch 36/100\n",
      "47/47 - 0s - loss: 666.5748 - val_loss: 587.2448 - 58ms/epoch - 1ms/step\n",
      "Epoch 37/100\n",
      "47/47 - 0s - loss: 667.8277 - val_loss: 600.3831 - 58ms/epoch - 1ms/step\n",
      "Epoch 38/100\n",
      "47/47 - 0s - loss: 665.3256 - val_loss: 575.6797 - 59ms/epoch - 1ms/step\n",
      "Epoch 39/100\n",
      "47/47 - 0s - loss: 665.3378 - val_loss: 587.3093 - 58ms/epoch - 1ms/step\n",
      "Epoch 40/100\n",
      "47/47 - 0s - loss: 666.2061 - val_loss: 581.1981 - 58ms/epoch - 1ms/step\n",
      "Epoch 41/100\n",
      "47/47 - 0s - loss: 665.3608 - val_loss: 607.5619 - 58ms/epoch - 1ms/step\n",
      "Epoch 42/100\n",
      "47/47 - 0s - loss: 663.7219 - val_loss: 593.9586 - 59ms/epoch - 1ms/step\n",
      "Epoch 43/100\n",
      "47/47 - 0s - loss: 665.6758 - val_loss: 621.4830 - 58ms/epoch - 1ms/step\n",
      "Epoch 44/100\n",
      "47/47 - 0s - loss: 664.6767 - val_loss: 581.8441 - 58ms/epoch - 1ms/step\n",
      "Epoch 45/100\n",
      "47/47 - 0s - loss: 663.7433 - val_loss: 590.6880 - 58ms/epoch - 1ms/step\n",
      "Epoch 46/100\n",
      "47/47 - 0s - loss: 663.9319 - val_loss: 596.3945 - 59ms/epoch - 1ms/step\n",
      "Epoch 47/100\n",
      "47/47 - 0s - loss: 663.5641 - val_loss: 598.6323 - 58ms/epoch - 1ms/step\n",
      "Epoch 48/100\n",
      "47/47 - 0s - loss: 663.6196 - val_loss: 619.8358 - 58ms/epoch - 1ms/step\n",
      "Epoch 49/100\n",
      "47/47 - 0s - loss: 663.9634 - val_loss: 618.0436 - 55ms/epoch - 1ms/step\n",
      "Epoch 50/100\n",
      "47/47 - 0s - loss: 661.7554 - val_loss: 624.3025 - 50ms/epoch - 1ms/step\n",
      "Epoch 51/100\n",
      "47/47 - 0s - loss: 662.9254 - val_loss: 624.9248 - 52ms/epoch - 1ms/step\n",
      "Epoch 52/100\n",
      "47/47 - 0s - loss: 663.5646 - val_loss: 604.7103 - 53ms/epoch - 1ms/step\n",
      "Epoch 53/100\n",
      "47/47 - 0s - loss: 661.0654 - val_loss: 602.9383 - 54ms/epoch - 1ms/step\n",
      "Epoch 54/100\n",
      "47/47 - 0s - loss: 660.8310 - val_loss: 595.3142 - 54ms/epoch - 1ms/step\n",
      "Epoch 55/100\n",
      "47/47 - 0s - loss: 659.6017 - val_loss: 590.9482 - 53ms/epoch - 1ms/step\n",
      "Epoch 56/100\n",
      "47/47 - 0s - loss: 662.2413 - val_loss: 556.9954 - 55ms/epoch - 1ms/step\n",
      "Epoch 57/100\n",
      "47/47 - 0s - loss: 659.4381 - val_loss: 635.1643 - 50ms/epoch - 1ms/step\n",
      "Epoch 58/100\n",
      "47/47 - 0s - loss: 658.6631 - val_loss: 628.0405 - 55ms/epoch - 1ms/step\n",
      "Epoch 59/100\n",
      "47/47 - 0s - loss: 658.2009 - val_loss: 627.8907 - 51ms/epoch - 1ms/step\n",
      "Epoch 60/100\n",
      "47/47 - 0s - loss: 658.5525 - val_loss: 641.2700 - 53ms/epoch - 1ms/step\n",
      "Epoch 61/100\n",
      "47/47 - 0s - loss: 660.1370 - val_loss: 599.7193 - 53ms/epoch - 1ms/step\n",
      "Epoch 62/100\n",
      "47/47 - 0s - loss: 659.0263 - val_loss: 656.1727 - 51ms/epoch - 1ms/step\n",
      "Epoch 63/100\n",
      "47/47 - 0s - loss: 657.4873 - val_loss: 602.9891 - 54ms/epoch - 1ms/step\n",
      "Epoch 64/100\n",
      "47/47 - 0s - loss: 657.3732 - val_loss: 575.1332 - 55ms/epoch - 1ms/step\n",
      "Epoch 65/100\n",
      "47/47 - 0s - loss: 655.6554 - val_loss: 561.3035 - 58ms/epoch - 1ms/step\n",
      "Epoch 66/100\n",
      "47/47 - 0s - loss: 656.6631 - val_loss: 614.6984 - 58ms/epoch - 1ms/step\n",
      "Epoch 67/100\n",
      "47/47 - 0s - loss: 654.8383 - val_loss: 603.0349 - 57ms/epoch - 1ms/step\n",
      "Epoch 68/100\n",
      "47/47 - 0s - loss: 653.2231 - val_loss: 612.0577 - 59ms/epoch - 1ms/step\n",
      "Epoch 69/100\n",
      "47/47 - 0s - loss: 654.2800 - val_loss: 613.5034 - 58ms/epoch - 1ms/step\n",
      "Epoch 70/100\n",
      "47/47 - 0s - loss: 654.1113 - val_loss: 607.6912 - 59ms/epoch - 1ms/step\n",
      "Epoch 71/100\n",
      "47/47 - 0s - loss: 653.0029 - val_loss: 619.0503 - 58ms/epoch - 1ms/step\n",
      "Epoch 72/100\n",
      "47/47 - 0s - loss: 651.3113 - val_loss: 624.6578 - 58ms/epoch - 1ms/step\n",
      "Epoch 73/100\n",
      "47/47 - 0s - loss: 653.3964 - val_loss: 671.3305 - 61ms/epoch - 1ms/step\n",
      "Epoch 74/100\n",
      "47/47 - 0s - loss: 649.8250 - val_loss: 592.1320 - 56ms/epoch - 1ms/step\n",
      "Epoch 75/100\n",
      "47/47 - 0s - loss: 648.7551 - val_loss: 559.9014 - 58ms/epoch - 1ms/step\n",
      "Epoch 76/100\n",
      "47/47 - 0s - loss: 649.7034 - val_loss: 576.5688 - 58ms/epoch - 1ms/step\n",
      "Epoch 77/100\n",
      "47/47 - 0s - loss: 650.9516 - val_loss: 549.3124 - 59ms/epoch - 1ms/step\n",
      "Epoch 78/100\n",
      "47/47 - 0s - loss: 647.3188 - val_loss: 643.3561 - 58ms/epoch - 1ms/step\n",
      "Epoch 79/100\n",
      "47/47 - 0s - loss: 646.7124 - val_loss: 629.5921 - 58ms/epoch - 1ms/step\n",
      "Epoch 80/100\n",
      "47/47 - 0s - loss: 647.5156 - val_loss: 613.6372 - 59ms/epoch - 1ms/step\n",
      "Epoch 81/100\n",
      "47/47 - 0s - loss: 648.6173 - val_loss: 574.3897 - 66ms/epoch - 1ms/step\n",
      "Epoch 82/100\n",
      "47/47 - 0s - loss: 645.7460 - val_loss: 651.8318 - 65ms/epoch - 1ms/step\n",
      "Epoch 83/100\n",
      "47/47 - 0s - loss: 644.3015 - val_loss: 571.3972 - 57ms/epoch - 1ms/step\n",
      "Epoch 84/100\n",
      "47/47 - 0s - loss: 644.1425 - val_loss: 551.2846 - 55ms/epoch - 1ms/step\n",
      "Epoch 85/100\n",
      "47/47 - 0s - loss: 642.9901 - val_loss: 599.9942 - 54ms/epoch - 1ms/step\n",
      "Epoch 86/100\n",
      "47/47 - 0s - loss: 641.6959 - val_loss: 625.0502 - 57ms/epoch - 1ms/step\n",
      "Epoch 87/100\n",
      "47/47 - 0s - loss: 641.3977 - val_loss: 600.2043 - 56ms/epoch - 1ms/step\n",
      "Epoch 88/100\n",
      "47/47 - 0s - loss: 640.8350 - val_loss: 619.5641 - 58ms/epoch - 1ms/step\n",
      "Epoch 89/100\n",
      "47/47 - 0s - loss: 641.8693 - val_loss: 526.6514 - 62ms/epoch - 1ms/step\n",
      "Epoch 90/100\n",
      "47/47 - 0s - loss: 639.1014 - val_loss: 547.7086 - 59ms/epoch - 1ms/step\n",
      "Epoch 91/100\n",
      "47/47 - 0s - loss: 636.8577 - val_loss: 593.9293 - 58ms/epoch - 1ms/step\n",
      "Epoch 92/100\n",
      "47/47 - 0s - loss: 638.5689 - val_loss: 601.8566 - 58ms/epoch - 1ms/step\n",
      "Epoch 93/100\n",
      "47/47 - 0s - loss: 638.7978 - val_loss: 572.1926 - 59ms/epoch - 1ms/step\n",
      "Epoch 94/100\n",
      "47/47 - 0s - loss: 634.2543 - val_loss: 587.3192 - 59ms/epoch - 1ms/step\n",
      "Epoch 95/100\n",
      "47/47 - 0s - loss: 636.1701 - val_loss: 572.7833 - 55ms/epoch - 1ms/step\n",
      "Epoch 96/100\n",
      "47/47 - 0s - loss: 636.8241 - val_loss: 564.5620 - 58ms/epoch - 1ms/step\n",
      "Epoch 97/100\n",
      "47/47 - 0s - loss: 635.9441 - val_loss: 569.4593 - 58ms/epoch - 1ms/step\n",
      "Epoch 98/100\n",
      "47/47 - 0s - loss: 631.5073 - val_loss: 587.4381 - 58ms/epoch - 1ms/step\n",
      "Epoch 99/100\n",
      "47/47 - 0s - loss: 631.7974 - val_loss: 549.1590 - 58ms/epoch - 1ms/step\n",
      "Epoch 100/100\n",
      "47/47 - 0s - loss: 629.9712 - val_loss: 557.9587 - 58ms/epoch - 1ms/step\n",
      "12/12 [==============================] - 0s 675us/step - loss: 557.9587\n",
      "Mean Squared Error on Test Set: 557.9586791992188\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('water_dataX1.csv', encoding='latin1')\n",
    "\n",
    "# Select the columns of interest\n",
    "selected_features = ['Temp', 'D.O. (mg/l)', 'PH', 'CONDUCTIVITY (µmhos/cm)', 'B.O.D. (mg/l)']\n",
    "df = df[selected_features]\n",
    "\n",
    "# Convert columns to numeric, handling errors\n",
    "df = df.apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "# Drop rows with NaN values\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "# Split the data into features (X) and target variable (y)\n",
    "X = df[['Temp', 'D.O. (mg/l)', 'PH', 'CONDUCTIVITY (µmhos/cm)']]\n",
    "y = df['B.O.D. (mg/l)']\n",
    "\n",
    "# Split data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardize the feature values\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Build the neural network model\n",
    "model = Sequential()\n",
    "model.add(Dense(64, input_dim=X_train.shape[1], activation='relu'))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(16, activation='linear'))\n",
    "\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train, y_train, epochs=100, batch_size=32, validation_data=(X_test, y_test), verbose=2)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "loss = model.evaluate(X_test, y_test)\n",
    "print(\"Mean Squared Error on Test Set:\", loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "47/47 - 0s - loss: 779.3911 - val_loss: 272.2262 - 386ms/epoch - 8ms/step\n",
      "Epoch 2/100\n",
      "47/47 - 0s - loss: 755.4587 - val_loss: 267.4700 - 54ms/epoch - 1ms/step\n",
      "Epoch 3/100\n",
      "47/47 - 0s - loss: 722.9793 - val_loss: 287.3288 - 45ms/epoch - 962us/step\n",
      "Epoch 4/100\n",
      "47/47 - 0s - loss: 695.6375 - val_loss: 343.5416 - 64ms/epoch - 1ms/step\n",
      "Epoch 5/100\n",
      "47/47 - 0s - loss: 685.0439 - val_loss: 417.4075 - 50ms/epoch - 1ms/step\n",
      "Epoch 6/100\n",
      "47/47 - 0s - loss: 680.6593 - val_loss: 406.0871 - 63ms/epoch - 1ms/step\n",
      "Epoch 7/100\n",
      "47/47 - 0s - loss: 680.0388 - val_loss: 448.7690 - 53ms/epoch - 1ms/step\n",
      "Epoch 8/100\n",
      "47/47 - 0s - loss: 676.7474 - val_loss: 447.5367 - 52ms/epoch - 1ms/step\n",
      "Epoch 9/100\n",
      "47/47 - 0s - loss: 676.2350 - val_loss: 471.3575 - 52ms/epoch - 1ms/step\n",
      "Epoch 10/100\n",
      "47/47 - 0s - loss: 674.1409 - val_loss: 488.9777 - 61ms/epoch - 1ms/step\n",
      "Epoch 11/100\n",
      "47/47 - 0s - loss: 674.1934 - val_loss: 412.1713 - 61ms/epoch - 1ms/step\n",
      "Epoch 12/100\n",
      "47/47 - 0s - loss: 670.7499 - val_loss: 563.8448 - 54ms/epoch - 1ms/step\n",
      "Epoch 13/100\n",
      "47/47 - 0s - loss: 671.7441 - val_loss: 402.1712 - 59ms/epoch - 1ms/step\n",
      "Epoch 14/100\n",
      "47/47 - 0s - loss: 668.0385 - val_loss: 476.8339 - 59ms/epoch - 1ms/step\n",
      "Epoch 15/100\n",
      "47/47 - 0s - loss: 669.3871 - val_loss: 489.0461 - 56ms/epoch - 1ms/step\n",
      "Epoch 16/100\n",
      "47/47 - 0s - loss: 669.6495 - val_loss: 429.9256 - 59ms/epoch - 1ms/step\n",
      "Epoch 17/100\n",
      "47/47 - 0s - loss: 667.2552 - val_loss: 545.0838 - 58ms/epoch - 1ms/step\n",
      "Epoch 18/100\n",
      "47/47 - 0s - loss: 667.5402 - val_loss: 575.7873 - 57ms/epoch - 1ms/step\n",
      "Epoch 19/100\n",
      "47/47 - 0s - loss: 666.7646 - val_loss: 506.3891 - 57ms/epoch - 1ms/step\n",
      "Epoch 20/100\n",
      "47/47 - 0s - loss: 665.4246 - val_loss: 541.1354 - 53ms/epoch - 1ms/step\n",
      "Epoch 21/100\n",
      "47/47 - 0s - loss: 664.7319 - val_loss: 562.0314 - 61ms/epoch - 1ms/step\n",
      "Epoch 22/100\n",
      "47/47 - 0s - loss: 663.5966 - val_loss: 564.6816 - 59ms/epoch - 1ms/step\n",
      "Epoch 23/100\n",
      "47/47 - 0s - loss: 660.0500 - val_loss: 541.3690 - 62ms/epoch - 1ms/step\n",
      "Epoch 24/100\n",
      "47/47 - 0s - loss: 662.5327 - val_loss: 551.1534 - 52ms/epoch - 1ms/step\n",
      "Epoch 25/100\n",
      "47/47 - 0s - loss: 658.5218 - val_loss: 553.1059 - 57ms/epoch - 1ms/step\n",
      "Epoch 26/100\n",
      "47/47 - 0s - loss: 665.2129 - val_loss: 559.7800 - 53ms/epoch - 1ms/step\n",
      "Epoch 27/100\n",
      "47/47 - 0s - loss: 657.7388 - val_loss: 531.1265 - 46ms/epoch - 973us/step\n",
      "Epoch 28/100\n",
      "47/47 - 0s - loss: 656.3668 - val_loss: 569.7841 - 58ms/epoch - 1ms/step\n",
      "Epoch 29/100\n",
      "47/47 - 0s - loss: 657.3267 - val_loss: 598.7930 - 53ms/epoch - 1ms/step\n",
      "Epoch 30/100\n",
      "47/47 - 0s - loss: 656.1446 - val_loss: 455.1853 - 56ms/epoch - 1ms/step\n",
      "Epoch 31/100\n",
      "47/47 - 0s - loss: 651.7233 - val_loss: 534.2026 - 56ms/epoch - 1ms/step\n",
      "Epoch 32/100\n",
      "47/47 - 0s - loss: 650.5540 - val_loss: 565.3602 - 58ms/epoch - 1ms/step\n",
      "Epoch 33/100\n",
      "47/47 - 0s - loss: 651.3245 - val_loss: 573.2869 - 56ms/epoch - 1ms/step\n",
      "Epoch 34/100\n",
      "47/47 - 0s - loss: 648.9854 - val_loss: 557.9490 - 52ms/epoch - 1ms/step\n",
      "Epoch 35/100\n",
      "47/47 - 0s - loss: 651.3097 - val_loss: 474.1325 - 52ms/epoch - 1ms/step\n",
      "Epoch 36/100\n",
      "47/47 - 0s - loss: 645.7026 - val_loss: 587.7968 - 59ms/epoch - 1ms/step\n",
      "Epoch 37/100\n",
      "47/47 - 0s - loss: 644.4235 - val_loss: 642.4403 - 51ms/epoch - 1ms/step\n",
      "Epoch 38/100\n",
      "47/47 - 0s - loss: 643.0363 - val_loss: 477.6001 - 56ms/epoch - 1ms/step\n",
      "Epoch 39/100\n",
      "47/47 - 0s - loss: 637.4585 - val_loss: 496.1203 - 54ms/epoch - 1ms/step\n",
      "Epoch 40/100\n",
      "47/47 - 0s - loss: 638.3259 - val_loss: 546.6260 - 58ms/epoch - 1ms/step\n",
      "Epoch 41/100\n",
      "47/47 - 0s - loss: 635.6505 - val_loss: 549.3262 - 58ms/epoch - 1ms/step\n",
      "Epoch 42/100\n",
      "47/47 - 0s - loss: 632.6843 - val_loss: 509.5331 - 58ms/epoch - 1ms/step\n",
      "Epoch 43/100\n",
      "47/47 - 0s - loss: 628.9772 - val_loss: 497.3752 - 55ms/epoch - 1ms/step\n",
      "Epoch 44/100\n",
      "47/47 - 0s - loss: 629.0024 - val_loss: 501.5960 - 47ms/epoch - 1ms/step\n",
      "Epoch 45/100\n",
      "47/47 - 0s - loss: 623.2281 - val_loss: 392.3473 - 68ms/epoch - 1ms/step\n",
      "Epoch 46/100\n",
      "47/47 - 0s - loss: 621.1625 - val_loss: 448.0581 - 58ms/epoch - 1ms/step\n",
      "Epoch 47/100\n",
      "47/47 - 0s - loss: 622.5848 - val_loss: 522.0728 - 58ms/epoch - 1ms/step\n",
      "Epoch 48/100\n",
      "47/47 - 0s - loss: 618.1102 - val_loss: 410.9692 - 59ms/epoch - 1ms/step\n",
      "Epoch 49/100\n",
      "47/47 - 0s - loss: 613.0691 - val_loss: 404.1017 - 58ms/epoch - 1ms/step\n",
      "Epoch 50/100\n",
      "47/47 - 0s - loss: 606.1929 - val_loss: 420.8731 - 59ms/epoch - 1ms/step\n",
      "Epoch 51/100\n",
      "47/47 - 0s - loss: 601.3375 - val_loss: 363.2169 - 58ms/epoch - 1ms/step\n",
      "Epoch 52/100\n",
      "47/47 - 0s - loss: 600.8810 - val_loss: 432.7597 - 58ms/epoch - 1ms/step\n",
      "Epoch 53/100\n",
      "47/47 - 0s - loss: 596.2736 - val_loss: 395.9459 - 59ms/epoch - 1ms/step\n",
      "Epoch 54/100\n",
      "47/47 - 0s - loss: 604.3300 - val_loss: 305.6458 - 64ms/epoch - 1ms/step\n",
      "Epoch 55/100\n",
      "47/47 - 0s - loss: 593.6218 - val_loss: 242.4325 - 58ms/epoch - 1ms/step\n",
      "Epoch 56/100\n",
      "47/47 - 0s - loss: 590.7518 - val_loss: 242.6486 - 55ms/epoch - 1ms/step\n",
      "Epoch 57/100\n",
      "47/47 - 0s - loss: 582.0956 - val_loss: 378.1105 - 50ms/epoch - 1ms/step\n",
      "Epoch 58/100\n",
      "47/47 - 0s - loss: 574.5738 - val_loss: 344.1301 - 53ms/epoch - 1ms/step\n",
      "Epoch 59/100\n",
      "47/47 - 0s - loss: 571.2242 - val_loss: 369.6088 - 55ms/epoch - 1ms/step\n",
      "Epoch 60/100\n",
      "47/47 - 0s - loss: 569.6327 - val_loss: 352.6154 - 56ms/epoch - 1ms/step\n",
      "Epoch 61/100\n",
      "47/47 - 0s - loss: 561.3730 - val_loss: 264.5776 - 55ms/epoch - 1ms/step\n",
      "Epoch 62/100\n",
      "47/47 - 0s - loss: 558.8574 - val_loss: 406.7287 - 54ms/epoch - 1ms/step\n",
      "Epoch 63/100\n",
      "47/47 - 0s - loss: 551.9975 - val_loss: 393.8600 - 54ms/epoch - 1ms/step\n",
      "Epoch 64/100\n",
      "47/47 - 0s - loss: 550.6884 - val_loss: 393.4511 - 57ms/epoch - 1ms/step\n",
      "Epoch 65/100\n",
      "47/47 - 0s - loss: 545.2283 - val_loss: 321.3619 - 57ms/epoch - 1ms/step\n",
      "Epoch 66/100\n",
      "47/47 - 0s - loss: 538.7728 - val_loss: 316.1466 - 46ms/epoch - 987us/step\n",
      "Epoch 67/100\n",
      "47/47 - 0s - loss: 534.9033 - val_loss: 389.7256 - 64ms/epoch - 1ms/step\n",
      "Epoch 68/100\n",
      "47/47 - 0s - loss: 542.2198 - val_loss: 269.0023 - 59ms/epoch - 1ms/step\n",
      "Epoch 69/100\n",
      "47/47 - 0s - loss: 525.2199 - val_loss: 213.8185 - 57ms/epoch - 1ms/step\n",
      "Epoch 70/100\n",
      "47/47 - 0s - loss: 524.8281 - val_loss: 344.7382 - 54ms/epoch - 1ms/step\n",
      "Epoch 71/100\n",
      "47/47 - 0s - loss: 517.8694 - val_loss: 214.1325 - 54ms/epoch - 1ms/step\n",
      "Epoch 72/100\n",
      "47/47 - 0s - loss: 528.9852 - val_loss: 483.6712 - 56ms/epoch - 1ms/step\n",
      "Epoch 73/100\n",
      "47/47 - 0s - loss: 521.1867 - val_loss: 487.0811 - 57ms/epoch - 1ms/step\n",
      "Epoch 74/100\n",
      "47/47 - 0s - loss: 527.2929 - val_loss: 215.6828 - 58ms/epoch - 1ms/step\n",
      "Epoch 75/100\n",
      "47/47 - 0s - loss: 509.0140 - val_loss: 221.4537 - 56ms/epoch - 1ms/step\n",
      "Epoch 76/100\n",
      "47/47 - 0s - loss: 509.3486 - val_loss: 512.5696 - 59ms/epoch - 1ms/step\n",
      "Epoch 77/100\n",
      "47/47 - 0s - loss: 503.1026 - val_loss: 226.8560 - 54ms/epoch - 1ms/step\n",
      "Epoch 78/100\n",
      "47/47 - 0s - loss: 504.5350 - val_loss: 390.2877 - 53ms/epoch - 1ms/step\n",
      "Epoch 79/100\n",
      "47/47 - 0s - loss: 497.7264 - val_loss: 165.4711 - 48ms/epoch - 1ms/step\n",
      "Epoch 80/100\n",
      "47/47 - 0s - loss: 498.8151 - val_loss: 173.0971 - 59ms/epoch - 1ms/step\n",
      "Epoch 81/100\n",
      "47/47 - 0s - loss: 492.2893 - val_loss: 220.6044 - 57ms/epoch - 1ms/step\n",
      "Epoch 82/100\n",
      "47/47 - 0s - loss: 489.8332 - val_loss: 217.8163 - 54ms/epoch - 1ms/step\n",
      "Epoch 83/100\n",
      "47/47 - 0s - loss: 491.6901 - val_loss: 270.5402 - 55ms/epoch - 1ms/step\n",
      "Epoch 84/100\n",
      "47/47 - 0s - loss: 497.3943 - val_loss: 219.3833 - 59ms/epoch - 1ms/step\n",
      "Epoch 85/100\n",
      "47/47 - 0s - loss: 490.9645 - val_loss: 359.5820 - 57ms/epoch - 1ms/step\n",
      "Epoch 86/100\n",
      "47/47 - 0s - loss: 484.3464 - val_loss: 191.2167 - 59ms/epoch - 1ms/step\n",
      "Epoch 87/100\n",
      "47/47 - 0s - loss: 494.1979 - val_loss: 187.5652 - 58ms/epoch - 1ms/step\n",
      "Epoch 88/100\n",
      "47/47 - 0s - loss: 489.1196 - val_loss: 269.0384 - 59ms/epoch - 1ms/step\n",
      "Epoch 89/100\n",
      "47/47 - 0s - loss: 484.4758 - val_loss: 252.1855 - 57ms/epoch - 1ms/step\n",
      "Epoch 90/100\n",
      "47/47 - 0s - loss: 482.4768 - val_loss: 264.0449 - 57ms/epoch - 1ms/step\n",
      "Epoch 91/100\n",
      "47/47 - 0s - loss: 489.3962 - val_loss: 215.6483 - 59ms/epoch - 1ms/step\n",
      "Epoch 92/100\n",
      "47/47 - 0s - loss: 483.8317 - val_loss: 457.8640 - 58ms/epoch - 1ms/step\n",
      "Epoch 93/100\n",
      "47/47 - 0s - loss: 481.1762 - val_loss: 260.8100 - 55ms/epoch - 1ms/step\n",
      "Epoch 94/100\n",
      "47/47 - 0s - loss: 481.0370 - val_loss: 213.9413 - 54ms/epoch - 1ms/step\n",
      "Epoch 95/100\n",
      "47/47 - 0s - loss: 481.0728 - val_loss: 198.0952 - 53ms/epoch - 1ms/step\n",
      "Epoch 96/100\n",
      "47/47 - 0s - loss: 479.2375 - val_loss: 305.0252 - 50ms/epoch - 1ms/step\n",
      "Epoch 97/100\n",
      "47/47 - 0s - loss: 478.3325 - val_loss: 283.1855 - 54ms/epoch - 1ms/step\n",
      "Epoch 98/100\n",
      "47/47 - 0s - loss: 474.7772 - val_loss: 225.5106 - 55ms/epoch - 1ms/step\n",
      "Epoch 99/100\n",
      "47/47 - 0s - loss: 476.6576 - val_loss: 225.0985 - 57ms/epoch - 1ms/step\n",
      "Epoch 100/100\n",
      "47/47 - 0s - loss: 478.5482 - val_loss: 272.1104 - 62ms/epoch - 1ms/step\n",
      "12/12 [==============================] - 0s 981us/step - loss: 272.1104\n",
      "Mean Squared Error on Test Set: 272.1103515625\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('water_dataX1.csv', encoding='latin1')\n",
    "\n",
    "# Select the columns of interest\n",
    "selected_features = ['Temp', 'D.O. (mg/l)', 'PH', 'CONDUCTIVITY (µmhos/cm)', 'B.O.D. (mg/l)']\n",
    "df = df[selected_features]\n",
    "\n",
    "# Convert columns to numeric, handling errors\n",
    "df = df.apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "# Drop rows with NaN values\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "# Split the data into features (X) and target variable (y)\n",
    "X = df[['Temp', 'D.O. (mg/l)', 'PH', 'CONDUCTIVITY (µmhos/cm)']]\n",
    "y = df['B.O.D. (mg/l)']\n",
    "\n",
    "# Split data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardize the feature values\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Build the neural network model\n",
    "model = Sequential()\n",
    "model.add(Dense(64, input_dim=X_train.shape[1], activation='relu'))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(16, activation='relu'))  # Change activation to 'relu'\n",
    "\n",
    "# Output layer with linear activation for regression\n",
    "model.add(Dense(1, activation='linear'))  # Change activation to 'linear'\n",
    "\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train, y_train, epochs=100, batch_size=32, validation_data=(X_test, y_test), verbose=2)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "loss = model.evaluate(X_test, y_test)\n",
    "print(\"Mean Squared Error on Test Set:\", loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "47/47 - 0s - loss: 1.1739 - val_loss: 3.0981 - 408ms/epoch - 9ms/step\n",
      "Epoch 2/100\n",
      "47/47 - 0s - loss: 0.5444 - val_loss: 1.1993 - 49ms/epoch - 1ms/step\n",
      "Epoch 3/100\n",
      "47/47 - 0s - loss: 0.4544 - val_loss: 0.7217 - 62ms/epoch - 1ms/step\n",
      "Epoch 4/100\n",
      "47/47 - 0s - loss: 0.4379 - val_loss: 0.6244 - 57ms/epoch - 1ms/step\n",
      "Epoch 5/100\n",
      "47/47 - 0s - loss: 0.4143 - val_loss: 0.5474 - 54ms/epoch - 1ms/step\n",
      "Epoch 6/100\n",
      "47/47 - 0s - loss: 0.4111 - val_loss: 0.4950 - 61ms/epoch - 1ms/step\n",
      "Epoch 7/100\n",
      "47/47 - 0s - loss: 0.4055 - val_loss: 0.4706 - 61ms/epoch - 1ms/step\n",
      "Epoch 8/100\n",
      "47/47 - 0s - loss: 0.3978 - val_loss: 0.4326 - 56ms/epoch - 1ms/step\n",
      "Epoch 9/100\n",
      "47/47 - 0s - loss: 0.3910 - val_loss: 0.5337 - 51ms/epoch - 1ms/step\n",
      "Epoch 10/100\n",
      "47/47 - 0s - loss: 0.3869 - val_loss: 0.4705 - 55ms/epoch - 1ms/step\n",
      "Epoch 11/100\n",
      "47/47 - 0s - loss: 0.3858 - val_loss: 0.4449 - 55ms/epoch - 1ms/step\n",
      "Epoch 12/100\n",
      "47/47 - 0s - loss: 0.3812 - val_loss: 0.4552 - 52ms/epoch - 1ms/step\n",
      "Epoch 13/100\n",
      "47/47 - 0s - loss: 0.3719 - val_loss: 0.4323 - 66ms/epoch - 1ms/step\n",
      "Epoch 14/100\n",
      "47/47 - 0s - loss: 0.3750 - val_loss: 0.3960 - 59ms/epoch - 1ms/step\n",
      "Epoch 15/100\n",
      "47/47 - 0s - loss: 0.3711 - val_loss: 0.4887 - 58ms/epoch - 1ms/step\n",
      "Epoch 16/100\n",
      "47/47 - 0s - loss: 0.3619 - val_loss: 0.4165 - 59ms/epoch - 1ms/step\n",
      "Epoch 17/100\n",
      "47/47 - 0s - loss: 0.3657 - val_loss: 0.4160 - 58ms/epoch - 1ms/step\n",
      "Epoch 18/100\n",
      "47/47 - 0s - loss: 0.3621 - val_loss: 0.4351 - 59ms/epoch - 1ms/step\n",
      "Epoch 19/100\n",
      "47/47 - 0s - loss: 0.3534 - val_loss: 0.3963 - 58ms/epoch - 1ms/step\n",
      "Epoch 20/100\n",
      "47/47 - 0s - loss: 0.3417 - val_loss: 0.5087 - 73ms/epoch - 2ms/step\n",
      "Epoch 21/100\n",
      "47/47 - 0s - loss: 0.3543 - val_loss: 0.3807 - 58ms/epoch - 1ms/step\n",
      "Epoch 22/100\n",
      "47/47 - 0s - loss: 0.3385 - val_loss: 0.4666 - 57ms/epoch - 1ms/step\n",
      "Epoch 23/100\n",
      "47/47 - 0s - loss: 0.3388 - val_loss: 0.3872 - 53ms/epoch - 1ms/step\n",
      "Epoch 24/100\n",
      "47/47 - 0s - loss: 0.3306 - val_loss: 0.3313 - 56ms/epoch - 1ms/step\n",
      "Epoch 25/100\n",
      "47/47 - 0s - loss: 0.3258 - val_loss: 0.3836 - 48ms/epoch - 1ms/step\n",
      "Epoch 26/100\n",
      "47/47 - 0s - loss: 0.3299 - val_loss: 0.3674 - 59ms/epoch - 1ms/step\n",
      "Epoch 27/100\n",
      "47/47 - 0s - loss: 0.3204 - val_loss: 0.3087 - 58ms/epoch - 1ms/step\n",
      "Epoch 28/100\n",
      "47/47 - 0s - loss: 0.3210 - val_loss: 0.4571 - 57ms/epoch - 1ms/step\n",
      "Epoch 29/100\n",
      "47/47 - 0s - loss: 0.3180 - val_loss: 0.3322 - 58ms/epoch - 1ms/step\n",
      "Epoch 30/100\n",
      "47/47 - 0s - loss: 0.3231 - val_loss: 0.3443 - 58ms/epoch - 1ms/step\n",
      "Epoch 31/100\n",
      "47/47 - 0s - loss: 0.3121 - val_loss: 0.3592 - 58ms/epoch - 1ms/step\n",
      "Epoch 32/100\n",
      "47/47 - 0s - loss: 0.3130 - val_loss: 0.2978 - 58ms/epoch - 1ms/step\n",
      "Epoch 33/100\n",
      "47/47 - 0s - loss: 0.3191 - val_loss: 0.3435 - 59ms/epoch - 1ms/step\n",
      "Epoch 34/100\n",
      "47/47 - 0s - loss: 0.3148 - val_loss: 0.3099 - 58ms/epoch - 1ms/step\n",
      "Epoch 35/100\n",
      "47/47 - 0s - loss: 0.3088 - val_loss: 0.3675 - 67ms/epoch - 1ms/step\n",
      "Epoch 36/100\n",
      "47/47 - 0s - loss: 0.3067 - val_loss: 0.3677 - 58ms/epoch - 1ms/step\n",
      "Epoch 37/100\n",
      "47/47 - 0s - loss: 0.3054 - val_loss: 0.3339 - 65ms/epoch - 1ms/step\n",
      "Epoch 38/100\n",
      "47/47 - 0s - loss: 0.3076 - val_loss: 0.3526 - 61ms/epoch - 1ms/step\n",
      "Epoch 39/100\n",
      "47/47 - 0s - loss: 0.3048 - val_loss: 0.3337 - 58ms/epoch - 1ms/step\n",
      "Epoch 40/100\n",
      "47/47 - 0s - loss: 0.3072 - val_loss: 0.3654 - 58ms/epoch - 1ms/step\n",
      "Epoch 41/100\n",
      "47/47 - 0s - loss: 0.3081 - val_loss: 0.3044 - 58ms/epoch - 1ms/step\n",
      "Epoch 42/100\n",
      "47/47 - 0s - loss: 0.3027 - val_loss: 0.3475 - 58ms/epoch - 1ms/step\n",
      "Epoch 43/100\n",
      "47/47 - 0s - loss: 0.2989 - val_loss: 0.3771 - 60ms/epoch - 1ms/step\n",
      "Epoch 44/100\n",
      "47/47 - 0s - loss: 0.2993 - val_loss: 0.3159 - 58ms/epoch - 1ms/step\n",
      "Epoch 45/100\n",
      "47/47 - 0s - loss: 0.3015 - val_loss: 0.3478 - 58ms/epoch - 1ms/step\n",
      "Epoch 46/100\n",
      "47/47 - 0s - loss: 0.2975 - val_loss: 0.3181 - 59ms/epoch - 1ms/step\n",
      "Epoch 47/100\n",
      "47/47 - 0s - loss: 0.2971 - val_loss: 0.3273 - 59ms/epoch - 1ms/step\n",
      "Epoch 48/100\n",
      "47/47 - 0s - loss: 0.2990 - val_loss: 0.3363 - 58ms/epoch - 1ms/step\n",
      "Epoch 49/100\n",
      "47/47 - 0s - loss: 0.2991 - val_loss: 0.3914 - 58ms/epoch - 1ms/step\n",
      "Epoch 50/100\n",
      "47/47 - 0s - loss: 0.3069 - val_loss: 0.3399 - 58ms/epoch - 1ms/step\n",
      "Epoch 51/100\n",
      "47/47 - 0s - loss: 0.2962 - val_loss: 0.3218 - 58ms/epoch - 1ms/step\n",
      "Epoch 52/100\n",
      "47/47 - 0s - loss: 0.2885 - val_loss: 0.3721 - 59ms/epoch - 1ms/step\n",
      "Epoch 53/100\n",
      "47/47 - 0s - loss: 0.2962 - val_loss: 0.3535 - 58ms/epoch - 1ms/step\n",
      "Epoch 54/100\n",
      "47/47 - 0s - loss: 0.2901 - val_loss: 0.3492 - 58ms/epoch - 1ms/step\n",
      "Epoch 55/100\n",
      "47/47 - 0s - loss: 0.2930 - val_loss: 0.3137 - 48ms/epoch - 1ms/step\n",
      "Epoch 56/100\n",
      "47/47 - 0s - loss: 0.2893 - val_loss: 0.3231 - 61ms/epoch - 1ms/step\n",
      "Epoch 57/100\n",
      "47/47 - 0s - loss: 0.2888 - val_loss: 0.3739 - 56ms/epoch - 1ms/step\n",
      "Epoch 58/100\n",
      "47/47 - 0s - loss: 0.2891 - val_loss: 0.2966 - 56ms/epoch - 1ms/step\n",
      "Epoch 59/100\n",
      "47/47 - 0s - loss: 0.2888 - val_loss: 0.3324 - 52ms/epoch - 1ms/step\n",
      "Epoch 60/100\n",
      "47/47 - 0s - loss: 0.2875 - val_loss: 0.3100 - 57ms/epoch - 1ms/step\n",
      "Epoch 61/100\n",
      "47/47 - 0s - loss: 0.2903 - val_loss: 0.3846 - 52ms/epoch - 1ms/step\n",
      "Epoch 62/100\n",
      "47/47 - 0s - loss: 0.2896 - val_loss: 0.3098 - 54ms/epoch - 1ms/step\n",
      "Epoch 63/100\n",
      "47/47 - 0s - loss: 0.2924 - val_loss: 0.4983 - 59ms/epoch - 1ms/step\n",
      "Epoch 64/100\n",
      "47/47 - 0s - loss: 0.2970 - val_loss: 0.3182 - 56ms/epoch - 1ms/step\n",
      "Epoch 65/100\n",
      "47/47 - 0s - loss: 0.2840 - val_loss: 0.3509 - 58ms/epoch - 1ms/step\n",
      "Epoch 66/100\n",
      "47/47 - 0s - loss: 0.3006 - val_loss: 0.3116 - 57ms/epoch - 1ms/step\n",
      "Epoch 67/100\n",
      "47/47 - 0s - loss: 0.2978 - val_loss: 0.3623 - 50ms/epoch - 1ms/step\n",
      "Epoch 68/100\n",
      "47/47 - 0s - loss: 0.2857 - val_loss: 0.3152 - 58ms/epoch - 1ms/step\n",
      "Epoch 69/100\n",
      "47/47 - 0s - loss: 0.2850 - val_loss: 0.3787 - 57ms/epoch - 1ms/step\n",
      "Epoch 70/100\n",
      "47/47 - 0s - loss: 0.2839 - val_loss: 0.3025 - 51ms/epoch - 1ms/step\n",
      "Epoch 71/100\n",
      "47/47 - 0s - loss: 0.2827 - val_loss: 0.3161 - 59ms/epoch - 1ms/step\n",
      "Epoch 72/100\n",
      "47/47 - 0s - loss: 0.2833 - val_loss: 0.3352 - 60ms/epoch - 1ms/step\n",
      "Epoch 73/100\n",
      "47/47 - 0s - loss: 0.2898 - val_loss: 0.3099 - 63ms/epoch - 1ms/step\n",
      "Epoch 74/100\n",
      "47/47 - 0s - loss: 0.2880 - val_loss: 0.3428 - 57ms/epoch - 1ms/step\n",
      "Epoch 75/100\n",
      "47/47 - 0s - loss: 0.2807 - val_loss: 0.3814 - 51ms/epoch - 1ms/step\n",
      "Epoch 76/100\n",
      "47/47 - 0s - loss: 0.2868 - val_loss: 0.3132 - 58ms/epoch - 1ms/step\n",
      "Epoch 77/100\n",
      "47/47 - 0s - loss: 0.2726 - val_loss: 0.3727 - 51ms/epoch - 1ms/step\n",
      "Epoch 78/100\n",
      "47/47 - 0s - loss: 0.2769 - val_loss: 0.3088 - 59ms/epoch - 1ms/step\n",
      "Epoch 79/100\n",
      "47/47 - 0s - loss: 0.2837 - val_loss: 0.3640 - 50ms/epoch - 1ms/step\n",
      "Epoch 80/100\n",
      "47/47 - 0s - loss: 0.2818 - val_loss: 0.3218 - 52ms/epoch - 1ms/step\n",
      "Epoch 81/100\n",
      "47/47 - 0s - loss: 0.2767 - val_loss: 0.3235 - 54ms/epoch - 1ms/step\n",
      "Epoch 82/100\n",
      "47/47 - 0s - loss: 0.2757 - val_loss: 0.3240 - 56ms/epoch - 1ms/step\n",
      "Epoch 83/100\n",
      "47/47 - 0s - loss: 0.2743 - val_loss: 0.3504 - 67ms/epoch - 1ms/step\n",
      "Epoch 84/100\n",
      "47/47 - 0s - loss: 0.2797 - val_loss: 0.3838 - 58ms/epoch - 1ms/step\n",
      "Epoch 85/100\n",
      "47/47 - 0s - loss: 0.2748 - val_loss: 0.4139 - 50ms/epoch - 1ms/step\n",
      "Epoch 86/100\n",
      "47/47 - 0s - loss: 0.2750 - val_loss: 0.3188 - 60ms/epoch - 1ms/step\n",
      "Epoch 87/100\n",
      "47/47 - 0s - loss: 0.2753 - val_loss: 0.3569 - 58ms/epoch - 1ms/step\n",
      "Epoch 88/100\n",
      "47/47 - 0s - loss: 0.2779 - val_loss: 0.3183 - 59ms/epoch - 1ms/step\n",
      "Epoch 89/100\n",
      "47/47 - 0s - loss: 0.2791 - val_loss: 0.3346 - 58ms/epoch - 1ms/step\n",
      "Epoch 90/100\n",
      "47/47 - 0s - loss: 0.2705 - val_loss: 0.2935 - 58ms/epoch - 1ms/step\n",
      "Epoch 91/100\n",
      "47/47 - 0s - loss: 0.2793 - val_loss: 0.3659 - 58ms/epoch - 1ms/step\n",
      "Epoch 92/100\n",
      "47/47 - 0s - loss: 0.2829 - val_loss: 0.3564 - 59ms/epoch - 1ms/step\n",
      "Epoch 93/100\n",
      "47/47 - 0s - loss: 0.2692 - val_loss: 0.3054 - 58ms/epoch - 1ms/step\n",
      "Epoch 94/100\n",
      "47/47 - 0s - loss: 0.2729 - val_loss: 0.2989 - 55ms/epoch - 1ms/step\n",
      "Epoch 95/100\n",
      "47/47 - 0s - loss: 0.2785 - val_loss: 0.3360 - 54ms/epoch - 1ms/step\n",
      "Epoch 96/100\n",
      "47/47 - 0s - loss: 0.2803 - val_loss: 0.3091 - 57ms/epoch - 1ms/step\n",
      "Epoch 97/100\n",
      "47/47 - 0s - loss: 0.2743 - val_loss: 0.3143 - 59ms/epoch - 1ms/step\n",
      "Epoch 98/100\n",
      "47/47 - 0s - loss: 0.2749 - val_loss: 0.3373 - 58ms/epoch - 1ms/step\n",
      "Epoch 99/100\n",
      "47/47 - 0s - loss: 0.2732 - val_loss: 0.3132 - 57ms/epoch - 1ms/step\n",
      "Epoch 100/100\n",
      "47/47 - 0s - loss: 0.2805 - val_loss: 0.3635 - 57ms/epoch - 1ms/step\n",
      "12/12 [==============================] - 0s 712us/step - loss: 0.3635\n",
      "Mean Squared Error on Test Set (with log-transformed target): 0.3634559214115143\n",
      "Mean Squared Error on Test Set (original scale): 257.15528301267915\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('water_dataX1.csv', encoding='latin1')\n",
    "\n",
    "# Select the columns of interest\n",
    "selected_features = ['Temp', 'D.O. (mg/l)', 'PH', 'CONDUCTIVITY (µmhos/cm)', 'B.O.D. (mg/l)']\n",
    "df = df[selected_features]\n",
    "\n",
    "# Convert columns to numeric, handling errors\n",
    "df = df.apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "# Drop rows with NaN values\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "# Split the data into features (X) and target variable (y)\n",
    "X = df[['Temp', 'D.O. (mg/l)', 'PH', 'CONDUCTIVITY (µmhos/cm)']]\n",
    "y = df['B.O.D. (mg/l)']\n",
    "\n",
    "# Apply a logarithmic transformation to the target variable\n",
    "y = np.log1p(y)  # Logarithmic transformation\n",
    "\n",
    "# Split data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardize the feature values\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Build the neural network model\n",
    "model = Sequential()\n",
    "model.add(Dense(64, input_dim=X_train.shape[1], activation='relu'))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(16, activation='relu'))  # Change activation to 'relu'\n",
    "\n",
    "# Output layer with linear activation for regression\n",
    "model.add(Dense(1, activation='linear'))  # Change activation to 'linear'\n",
    "\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train, y_train, epochs=100, batch_size=32, validation_data=(X_test, y_test), verbose=2)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "loss = model.evaluate(X_test, y_test)\n",
    "print(\"Mean Squared Error on Test Set (with log-transformed target):\", loss)\n",
    "\n",
    "y_pred_original = np.expm1(y_pred).flatten()\n",
    "y_test_original = np.expm1(y_test.to_numpy()).flatten()\n",
    "\n",
    "# Calculate the mean squared error on the original scale.\n",
    "mse_original_scale = np.mean(np.square(y_pred_original - y_test_original))\n",
    "print(\"Mean Squared Error on Test Set (original scale):\", mse_original_scale)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "46/46 - 1s - loss: 1.1999 - val_loss: 0.7356 - 527ms/epoch - 11ms/step\n",
      "Epoch 2/100\n",
      "46/46 - 0s - loss: 0.4785 - val_loss: 0.3264 - 81ms/epoch - 2ms/step\n",
      "Epoch 3/100\n",
      "46/46 - 0s - loss: 0.3717 - val_loss: 0.3172 - 77ms/epoch - 2ms/step\n",
      "Epoch 4/100\n",
      "46/46 - 0s - loss: 0.3443 - val_loss: 0.2982 - 67ms/epoch - 1ms/step\n",
      "Epoch 5/100\n",
      "46/46 - 0s - loss: 0.3302 - val_loss: 0.3149 - 69ms/epoch - 2ms/step\n",
      "Epoch 6/100\n",
      "46/46 - 0s - loss: 0.3234 - val_loss: 0.2931 - 54ms/epoch - 1ms/step\n",
      "Epoch 7/100\n",
      "46/46 - 0s - loss: 0.3184 - val_loss: 0.2920 - 57ms/epoch - 1ms/step\n",
      "Epoch 8/100\n",
      "46/46 - 0s - loss: 0.3156 - val_loss: 0.2963 - 54ms/epoch - 1ms/step\n",
      "Epoch 9/100\n",
      "46/46 - 0s - loss: 0.3127 - val_loss: 0.2961 - 51ms/epoch - 1ms/step\n",
      "Epoch 10/100\n",
      "46/46 - 0s - loss: 0.3112 - val_loss: 0.2974 - 51ms/epoch - 1ms/step\n",
      "Epoch 11/100\n",
      "46/46 - 0s - loss: 0.3098 - val_loss: 0.2694 - 52ms/epoch - 1ms/step\n",
      "Epoch 12/100\n",
      "46/46 - 0s - loss: 0.3012 - val_loss: 0.2866 - 53ms/epoch - 1ms/step\n",
      "Epoch 13/100\n",
      "46/46 - 0s - loss: 0.3052 - val_loss: 0.2893 - 54ms/epoch - 1ms/step\n",
      "Epoch 14/100\n",
      "46/46 - 0s - loss: 0.3039 - val_loss: 0.2642 - 53ms/epoch - 1ms/step\n",
      "Epoch 15/100\n",
      "46/46 - 0s - loss: 0.2934 - val_loss: 0.2782 - 54ms/epoch - 1ms/step\n",
      "Epoch 16/100\n",
      "46/46 - 0s - loss: 0.2965 - val_loss: 0.2802 - 50ms/epoch - 1ms/step\n",
      "Epoch 17/100\n",
      "46/46 - 0s - loss: 0.2885 - val_loss: 0.2567 - 56ms/epoch - 1ms/step\n",
      "Epoch 18/100\n",
      "46/46 - 0s - loss: 0.2875 - val_loss: 0.2511 - 54ms/epoch - 1ms/step\n",
      "Epoch 19/100\n",
      "46/46 - 0s - loss: 0.2821 - val_loss: 0.2469 - 53ms/epoch - 1ms/step\n",
      "Epoch 20/100\n",
      "46/46 - 0s - loss: 0.2789 - val_loss: 0.2529 - 53ms/epoch - 1ms/step\n",
      "Epoch 21/100\n",
      "46/46 - 0s - loss: 0.2713 - val_loss: 0.2472 - 51ms/epoch - 1ms/step\n",
      "Epoch 22/100\n",
      "46/46 - 0s - loss: 0.2681 - val_loss: 0.2375 - 50ms/epoch - 1ms/step\n",
      "Epoch 23/100\n",
      "46/46 - 0s - loss: 0.2682 - val_loss: 0.2439 - 44ms/epoch - 953us/step\n",
      "Epoch 24/100\n",
      "46/46 - 0s - loss: 0.2677 - val_loss: 0.2338 - 58ms/epoch - 1ms/step\n",
      "Epoch 25/100\n",
      "46/46 - 0s - loss: 0.2614 - val_loss: 0.2307 - 51ms/epoch - 1ms/step\n",
      "Epoch 26/100\n",
      "46/46 - 0s - loss: 0.2577 - val_loss: 0.2334 - 54ms/epoch - 1ms/step\n",
      "Epoch 27/100\n",
      "46/46 - 0s - loss: 0.2613 - val_loss: 0.2242 - 51ms/epoch - 1ms/step\n",
      "Epoch 28/100\n",
      "46/46 - 0s - loss: 0.2540 - val_loss: 0.2228 - 55ms/epoch - 1ms/step\n",
      "Epoch 29/100\n",
      "46/46 - 0s - loss: 0.2553 - val_loss: 0.2438 - 59ms/epoch - 1ms/step\n",
      "Epoch 30/100\n",
      "46/46 - 0s - loss: 0.2533 - val_loss: 0.2225 - 52ms/epoch - 1ms/step\n",
      "Epoch 31/100\n",
      "46/46 - 0s - loss: 0.2546 - val_loss: 0.2258 - 53ms/epoch - 1ms/step\n",
      "Epoch 32/100\n",
      "46/46 - 0s - loss: 0.2490 - val_loss: 0.2193 - 53ms/epoch - 1ms/step\n",
      "Epoch 33/100\n",
      "46/46 - 0s - loss: 0.2580 - val_loss: 0.2155 - 53ms/epoch - 1ms/step\n",
      "Epoch 34/100\n",
      "46/46 - 0s - loss: 0.2514 - val_loss: 0.2201 - 51ms/epoch - 1ms/step\n",
      "Epoch 35/100\n",
      "46/46 - 0s - loss: 0.2486 - val_loss: 0.2254 - 53ms/epoch - 1ms/step\n",
      "Epoch 36/100\n",
      "46/46 - 0s - loss: 0.2517 - val_loss: 0.2172 - 54ms/epoch - 1ms/step\n",
      "Epoch 37/100\n",
      "46/46 - 0s - loss: 0.2518 - val_loss: 0.2216 - 53ms/epoch - 1ms/step\n",
      "Epoch 38/100\n",
      "46/46 - 0s - loss: 0.2477 - val_loss: 0.2411 - 53ms/epoch - 1ms/step\n",
      "Epoch 39/100\n",
      "46/46 - 0s - loss: 0.2478 - val_loss: 0.2173 - 52ms/epoch - 1ms/step\n",
      "Epoch 40/100\n",
      "46/46 - 0s - loss: 0.2450 - val_loss: 0.2226 - 53ms/epoch - 1ms/step\n",
      "Epoch 41/100\n",
      "46/46 - 0s - loss: 0.2416 - val_loss: 0.2244 - 51ms/epoch - 1ms/step\n",
      "Epoch 42/100\n",
      "46/46 - 0s - loss: 0.2436 - val_loss: 0.2279 - 49ms/epoch - 1ms/step\n",
      "Epoch 43/100\n",
      "46/46 - 0s - loss: 0.2475 - val_loss: 0.2244 - 50ms/epoch - 1ms/step\n",
      "Epoch 44/100\n",
      "46/46 - 0s - loss: 0.2410 - val_loss: 0.2132 - 42ms/epoch - 912us/step\n",
      "Epoch 45/100\n",
      "46/46 - 0s - loss: 0.2474 - val_loss: 0.2152 - 51ms/epoch - 1ms/step\n",
      "Epoch 46/100\n",
      "46/46 - 0s - loss: 0.2438 - val_loss: 0.2217 - 57ms/epoch - 1ms/step\n",
      "Epoch 47/100\n",
      "46/46 - 0s - loss: 0.2402 - val_loss: 0.2065 - 52ms/epoch - 1ms/step\n",
      "Epoch 48/100\n",
      "46/46 - 0s - loss: 0.2394 - val_loss: 0.2093 - 53ms/epoch - 1ms/step\n",
      "Epoch 49/100\n",
      "46/46 - 0s - loss: 0.2374 - val_loss: 0.2150 - 56ms/epoch - 1ms/step\n",
      "Epoch 50/100\n",
      "46/46 - 0s - loss: 0.2380 - val_loss: 0.2140 - 51ms/epoch - 1ms/step\n",
      "Epoch 51/100\n",
      "46/46 - 0s - loss: 0.2366 - val_loss: 0.2281 - 43ms/epoch - 937us/step\n",
      "Epoch 52/100\n",
      "46/46 - 0s - loss: 0.2404 - val_loss: 0.2108 - 52ms/epoch - 1ms/step\n",
      "Epoch 53/100\n",
      "46/46 - 0s - loss: 0.2394 - val_loss: 0.2105 - 44ms/epoch - 949us/step\n",
      "Epoch 54/100\n",
      "46/46 - 0s - loss: 0.2347 - val_loss: 0.2175 - 59ms/epoch - 1ms/step\n",
      "Epoch 55/100\n",
      "46/46 - 0s - loss: 0.2365 - val_loss: 0.2250 - 58ms/epoch - 1ms/step\n",
      "Epoch 56/100\n",
      "46/46 - 0s - loss: 0.2354 - val_loss: 0.2184 - 56ms/epoch - 1ms/step\n",
      "Epoch 57/100\n",
      "46/46 - 0s - loss: 0.2360 - val_loss: 0.2073 - 52ms/epoch - 1ms/step\n",
      "Epoch 58/100\n",
      "46/46 - 0s - loss: 0.2351 - val_loss: 0.2128 - 56ms/epoch - 1ms/step\n",
      "Epoch 59/100\n",
      "46/46 - 0s - loss: 0.2358 - val_loss: 0.2142 - 50ms/epoch - 1ms/step\n",
      "Epoch 60/100\n",
      "46/46 - 0s - loss: 0.2306 - val_loss: 0.2109 - 53ms/epoch - 1ms/step\n",
      "Epoch 61/100\n",
      "46/46 - 0s - loss: 0.2370 - val_loss: 0.2255 - 52ms/epoch - 1ms/step\n",
      "Epoch 62/100\n",
      "46/46 - 0s - loss: 0.2342 - val_loss: 0.2122 - 53ms/epoch - 1ms/step\n",
      "Epoch 63/100\n",
      "46/46 - 0s - loss: 0.2352 - val_loss: 0.2184 - 55ms/epoch - 1ms/step\n",
      "Epoch 64/100\n",
      "46/46 - 0s - loss: 0.2311 - val_loss: 0.2143 - 51ms/epoch - 1ms/step\n",
      "Epoch 65/100\n",
      "46/46 - 0s - loss: 0.2344 - val_loss: 0.2142 - 52ms/epoch - 1ms/step\n",
      "Epoch 66/100\n",
      "46/46 - 0s - loss: 0.2324 - val_loss: 0.2148 - 51ms/epoch - 1ms/step\n",
      "Epoch 67/100\n",
      "46/46 - 0s - loss: 0.2346 - val_loss: 0.2183 - 57ms/epoch - 1ms/step\n",
      "Epoch 68/100\n",
      "46/46 - 0s - loss: 0.2336 - val_loss: 0.2108 - 53ms/epoch - 1ms/step\n",
      "Epoch 69/100\n",
      "46/46 - 0s - loss: 0.2311 - val_loss: 0.2220 - 51ms/epoch - 1ms/step\n",
      "Epoch 70/100\n",
      "46/46 - 0s - loss: 0.2403 - val_loss: 0.2295 - 54ms/epoch - 1ms/step\n",
      "Epoch 71/100\n",
      "46/46 - 0s - loss: 0.2342 - val_loss: 0.2307 - 50ms/epoch - 1ms/step\n",
      "Epoch 72/100\n",
      "46/46 - 0s - loss: 0.2334 - val_loss: 0.2157 - 54ms/epoch - 1ms/step\n",
      "Epoch 73/100\n",
      "46/46 - 0s - loss: 0.2300 - val_loss: 0.2118 - 53ms/epoch - 1ms/step\n",
      "Epoch 74/100\n",
      "46/46 - 0s - loss: 0.2281 - val_loss: 0.2145 - 50ms/epoch - 1ms/step\n",
      "Epoch 75/100\n",
      "46/46 - 0s - loss: 0.2295 - val_loss: 0.2028 - 51ms/epoch - 1ms/step\n",
      "Epoch 76/100\n",
      "46/46 - 0s - loss: 0.2296 - val_loss: 0.2197 - 51ms/epoch - 1ms/step\n",
      "Epoch 77/100\n",
      "46/46 - 0s - loss: 0.2313 - val_loss: 0.2111 - 54ms/epoch - 1ms/step\n",
      "Epoch 78/100\n",
      "46/46 - 0s - loss: 0.2284 - val_loss: 0.2180 - 52ms/epoch - 1ms/step\n",
      "Epoch 79/100\n",
      "46/46 - 0s - loss: 0.2274 - val_loss: 0.2132 - 51ms/epoch - 1ms/step\n",
      "Epoch 80/100\n",
      "46/46 - 0s - loss: 0.2360 - val_loss: 0.2156 - 53ms/epoch - 1ms/step\n",
      "Epoch 81/100\n",
      "46/46 - 0s - loss: 0.2272 - val_loss: 0.2187 - 56ms/epoch - 1ms/step\n",
      "Epoch 82/100\n",
      "46/46 - 0s - loss: 0.2308 - val_loss: 0.2134 - 53ms/epoch - 1ms/step\n",
      "Epoch 83/100\n",
      "46/46 - 0s - loss: 0.2299 - val_loss: 0.2141 - 67ms/epoch - 1ms/step\n",
      "Epoch 84/100\n",
      "46/46 - 0s - loss: 0.2267 - val_loss: 0.2199 - 52ms/epoch - 1ms/step\n",
      "Epoch 85/100\n",
      "46/46 - 0s - loss: 0.2272 - val_loss: 0.2105 - 43ms/epoch - 945us/step\n",
      "Epoch 86/100\n",
      "46/46 - 0s - loss: 0.2231 - val_loss: 0.2174 - 48ms/epoch - 1ms/step\n",
      "Epoch 87/100\n",
      "46/46 - 0s - loss: 0.2250 - val_loss: 0.2254 - 52ms/epoch - 1ms/step\n",
      "Epoch 88/100\n",
      "46/46 - 0s - loss: 0.2233 - val_loss: 0.2227 - 48ms/epoch - 1ms/step\n",
      "Epoch 89/100\n",
      "46/46 - 0s - loss: 0.2243 - val_loss: 0.2138 - 50ms/epoch - 1ms/step\n",
      "Epoch 90/100\n",
      "46/46 - 0s - loss: 0.2230 - val_loss: 0.2104 - 58ms/epoch - 1ms/step\n",
      "Epoch 91/100\n",
      "46/46 - 0s - loss: 0.2260 - val_loss: 0.2196 - 42ms/epoch - 913us/step\n",
      "Epoch 92/100\n",
      "46/46 - 0s - loss: 0.2264 - val_loss: 0.2145 - 57ms/epoch - 1ms/step\n",
      "Epoch 93/100\n",
      "46/46 - 0s - loss: 0.2256 - val_loss: 0.2091 - 57ms/epoch - 1ms/step\n",
      "Epoch 94/100\n",
      "46/46 - 0s - loss: 0.2261 - val_loss: 0.2087 - 54ms/epoch - 1ms/step\n",
      "Epoch 95/100\n",
      "46/46 - 0s - loss: 0.2256 - val_loss: 0.2121 - 51ms/epoch - 1ms/step\n",
      "Epoch 96/100\n",
      "46/46 - 0s - loss: 0.2215 - val_loss: 0.2182 - 56ms/epoch - 1ms/step\n",
      "Epoch 97/100\n",
      "46/46 - 0s - loss: 0.2246 - val_loss: 0.2253 - 59ms/epoch - 1ms/step\n",
      "Epoch 98/100\n",
      "46/46 - 0s - loss: 0.2301 - val_loss: 0.2239 - 56ms/epoch - 1ms/step\n",
      "Epoch 99/100\n",
      "46/46 - 0s - loss: 0.2218 - val_loss: 0.2290 - 47ms/epoch - 1ms/step\n",
      "Epoch 100/100\n",
      "46/46 - 0s - loss: 0.2224 - val_loss: 0.2243 - 59ms/epoch - 1ms/step\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.2243\n",
      "Mean Squared Error on Test Set (with log-transformed target): 0.22430145740509033\n",
      "Mean Squared Error on Test Set (original scale): 22.56512405893242\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('water_dataX1.csv', encoding='latin1')\n",
    "\n",
    "# Select the columns of interest\n",
    "selected_features = ['Temp', 'D.O. (mg/l)', 'PH', 'CONDUCTIVITY (µmhos/cm)', 'B.O.D. (mg/l)']\n",
    "df = df[selected_features]\n",
    "\n",
    "# Convert columns to numeric, handling errors\n",
    "df = df.apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "# Drop rows with NaN values\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "# Remove rows with 'B.O.D. (mg/l)' greater than 300\n",
    "df = df[df['B.O.D. (mg/l)'] <= 100]\n",
    "\n",
    "# Split the data into features (X) and target variable (y)\n",
    "X = df[['Temp', 'D.O. (mg/l)', 'PH', 'CONDUCTIVITY (µmhos/cm)']]\n",
    "y = df['B.O.D. (mg/l)']\n",
    "\n",
    "# Apply a logarithmic transformation to the target variable\n",
    "y = np.log1p(y)  # Logarithmic transformation\n",
    "\n",
    "# Split data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardize the feature values\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Build the neural network model\n",
    "model = Sequential()\n",
    "model.add(Dense(64, input_dim=X_train.shape[1], activation='relu'))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(16, activation='relu'))  # Change activation to 'relu'\n",
    "\n",
    "# Output layer with linear activation for regression\n",
    "model.add(Dense(1, activation='linear'))  # Change activation to 'linear'\n",
    "\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train, y_train, epochs=100, batch_size=32, validation_data=(X_test, y_test), verbose=2)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "loss = model.evaluate(X_test, y_test)\n",
    "print(\"Mean Squared Error on Test Set (with log-transformed target):\", loss)\n",
    "\n",
    "# Inverse log transformation for predictions and true values\n",
    "y_pred_original = np.expm1(y_pred).flatten()\n",
    "y_test_original = np.expm1(y_test).to_numpy().flatten()\n",
    "\n",
    "# Calculate the mean squared error on the original scale.\n",
    "mse_original_scale = np.mean(np.square(y_pred_original - y_test_original))\n",
    "print(\"Mean Squared Error on Test Set (original scale):\", mse_original_scale)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 1838 entries, 0 to 1859\n",
      "Data columns (total 5 columns):\n",
      " #   Column                   Non-Null Count  Dtype  \n",
      "---  ------                   --------------  -----  \n",
      " 0   Temp                     1838 non-null   float64\n",
      " 1   D.O. (mg/l)              1838 non-null   float64\n",
      " 2   PH                       1838 non-null   float64\n",
      " 3   CONDUCTIVITY (µmhos/cm)  1838 non-null   float64\n",
      " 4   B.O.D. (mg/l)            1838 non-null   float64\n",
      "dtypes: float64(5)\n",
      "memory usage: 86.2 KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted B.O.D. (mg/l): 2.321905\n"
     ]
    }
   ],
   "source": [
    "# Create a dictionary with input feature values\n",
    "input_data = {\n",
    "    'Temp': [25.0],  # Replace with the desired value\n",
    "    'D.O. (mg/l)': [7.0],  # Replace with the desired value\n",
    "    'PH': [6.5],  # Replace with the desired value\n",
    "    'CONDUCTIVITY (µmhos/cm)': [511.0]  # Replace with the desired value\n",
    "}\n",
    "\n",
    "# Create a DataFrame from the input data\n",
    "input_df = pd.DataFrame(input_data)\n",
    "\n",
    "# Standardize the input data using the same scaler used for training\n",
    "input_scaled = scaler.transform(input_df)\n",
    "\n",
    "# Predict B.O.D. (B.O.D. (mg/l)) using the trained model\n",
    "predicted_log_bod = model.predict(input_scaled)\n",
    "\n",
    "# Inverse log transformation to get the predicted value on the original scale\n",
    "predicted_bod = np.expm1(predicted_log_bod)\n",
    "\n",
    "# Print the predicted B.O.D. value\n",
    "print(\"Predicted B.O.D. (mg/l):\", predicted_bod[0][0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# potability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "56/56 [==============================] - 1s 4ms/step - loss: 58.7946 - accuracy: 0.5017 - val_loss: 8.1788 - val_accuracy: 0.3991\n",
      "Epoch 2/20\n",
      "56/56 [==============================] - 0s 2ms/step - loss: 13.5391 - accuracy: 0.5303 - val_loss: 9.5390 - val_accuracy: 0.6099\n",
      "Epoch 3/20\n",
      "56/56 [==============================] - 0s 1ms/step - loss: 12.3905 - accuracy: 0.5292 - val_loss: 28.4504 - val_accuracy: 0.3924\n",
      "Epoch 4/20\n",
      "56/56 [==============================] - 0s 2ms/step - loss: 16.7643 - accuracy: 0.4989 - val_loss: 22.5585 - val_accuracy: 0.6076\n",
      "Epoch 5/20\n",
      "56/56 [==============================] - 0s 1ms/step - loss: 14.7315 - accuracy: 0.5292 - val_loss: 10.6032 - val_accuracy: 0.3924\n",
      "Epoch 6/20\n",
      "56/56 [==============================] - 0s 2ms/step - loss: 9.2744 - accuracy: 0.5191 - val_loss: 3.2892 - val_accuracy: 0.6076\n",
      "Epoch 7/20\n",
      "56/56 [==============================] - 0s 2ms/step - loss: 5.0280 - accuracy: 0.5258 - val_loss: 2.8310 - val_accuracy: 0.3924\n",
      "Epoch 8/20\n",
      "56/56 [==============================] - 0s 1ms/step - loss: 7.2075 - accuracy: 0.5107 - val_loss: 6.4405 - val_accuracy: 0.6099\n",
      "Epoch 9/20\n",
      "56/56 [==============================] - 0s 1ms/step - loss: 6.5064 - accuracy: 0.5202 - val_loss: 1.0027 - val_accuracy: 0.3924\n",
      "Epoch 10/20\n",
      "56/56 [==============================] - 0s 2ms/step - loss: 7.9136 - accuracy: 0.5168 - val_loss: 0.9167 - val_accuracy: 0.3946\n",
      "Epoch 11/20\n",
      "56/56 [==============================] - 0s 1ms/step - loss: 4.8763 - accuracy: 0.5168 - val_loss: 5.7100 - val_accuracy: 0.6076\n",
      "Epoch 12/20\n",
      "56/56 [==============================] - 0s 2ms/step - loss: 5.0074 - accuracy: 0.5163 - val_loss: 5.0703 - val_accuracy: 0.3901\n",
      "Epoch 13/20\n",
      "56/56 [==============================] - 0s 1ms/step - loss: 5.6207 - accuracy: 0.5370 - val_loss: 6.5568 - val_accuracy: 0.3924\n",
      "Epoch 14/20\n",
      "56/56 [==============================] - 0s 2ms/step - loss: 4.4699 - accuracy: 0.5146 - val_loss: 3.5420 - val_accuracy: 0.6076\n",
      "Epoch 15/20\n",
      "56/56 [==============================] - 0s 1ms/step - loss: 5.8381 - accuracy: 0.5135 - val_loss: 1.6196 - val_accuracy: 0.3924\n",
      "Epoch 16/20\n",
      "56/56 [==============================] - 0s 2ms/step - loss: 3.3936 - accuracy: 0.5331 - val_loss: 4.5624 - val_accuracy: 0.6076\n",
      "Epoch 17/20\n",
      "56/56 [==============================] - 0s 2ms/step - loss: 3.3441 - accuracy: 0.5196 - val_loss: 0.6899 - val_accuracy: 0.5650\n",
      "Epoch 18/20\n",
      "56/56 [==============================] - 0s 2ms/step - loss: 1.9453 - accuracy: 0.5202 - val_loss: 9.6789 - val_accuracy: 0.6076\n",
      "Epoch 19/20\n",
      "56/56 [==============================] - 0s 2ms/step - loss: 3.0722 - accuracy: 0.5342 - val_loss: 1.4901 - val_accuracy: 0.6099\n",
      "Epoch 20/20\n",
      "56/56 [==============================] - 0s 2ms/step - loss: 2.2300 - accuracy: 0.5286 - val_loss: 7.5035 - val_accuracy: 0.3924\n",
      "Accuracy: 0.4021543985637343\n",
      "Confusion Matrix:\n",
      " [[  0 333]\n",
      " [  0 224]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00       333\n",
      "           1       0.40      1.00      0.57       224\n",
      "\n",
      "    accuracy                           0.40       557\n",
      "   macro avg       0.20      0.50      0.29       557\n",
      "weighted avg       0.16      0.40      0.23       557\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\AARON\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\AARON\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\AARON\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "df = pd.read_excel('waterdf1.xlsx')\n",
    "X = df.drop('Potability', axis=1)\n",
    "y = df['Potability']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Input(shape=(X_train.shape[1],)),  # Input layer\n",
    "    tf.keras.layers.Dense(128, activation='relu'),    # Hidden layer with 128 neurons and ReLU activation\n",
    "    tf.keras.layers.Dense(64, activation='relu'),     # Hidden layer with 64 neurons and ReLU activation\n",
    "    tf.keras.layers.Dense(32, activation='relu'),     # Hidden layer with 32 neurons and ReLU activation\n",
    "    tf.keras.layers.Dense(16, activation='relu'),     # Hidden layer with 16 neurons and ReLU activation\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')    # Output layer with 1 neuron and sigmoid activation\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model.fit(X_train, y_train, epochs=20, batch_size=32, verbose=1, validation_split=0.2)\n",
    "y_pred_probs = model.predict(X_test)\n",
    "y_pred = (y_pred_probs > 0.5).astype(int)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "classification_rep = classification_report(y_test, y_pred)\n",
    "\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Confusion Matrix:\\n\", conf_matrix)\n",
    "print(\"Classification Report:\\n\", classification_rep)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "56/56 [==============================] - 1s 3ms/step - loss: 91.8291 - accuracy: 0.5152 - val_loss: 21.6077 - val_accuracy: 0.3946\n",
      "Epoch 2/20\n",
      "56/56 [==============================] - 0s 1ms/step - loss: 11.5990 - accuracy: 0.5297 - val_loss: 4.6482 - val_accuracy: 0.4350\n",
      "Epoch 3/20\n",
      "56/56 [==============================] - 0s 2ms/step - loss: 16.8959 - accuracy: 0.5219 - val_loss: 32.3924 - val_accuracy: 0.6076\n",
      "Epoch 4/20\n",
      "56/56 [==============================] - 0s 2ms/step - loss: 11.8724 - accuracy: 0.5550 - val_loss: 5.3264 - val_accuracy: 0.4193\n",
      "Epoch 5/20\n",
      "56/56 [==============================] - 0s 2ms/step - loss: 11.4471 - accuracy: 0.5219 - val_loss: 10.4027 - val_accuracy: 0.6099\n",
      "Epoch 6/20\n",
      "56/56 [==============================] - 0s 2ms/step - loss: 12.3046 - accuracy: 0.5286 - val_loss: 9.3707 - val_accuracy: 0.3969\n",
      "Epoch 7/20\n",
      "56/56 [==============================] - 0s 2ms/step - loss: 10.7147 - accuracy: 0.5062 - val_loss: 3.2811 - val_accuracy: 0.6076\n",
      "Epoch 8/20\n",
      "56/56 [==============================] - 0s 2ms/step - loss: 5.8589 - accuracy: 0.5185 - val_loss: 28.6132 - val_accuracy: 0.3924\n",
      "Epoch 9/20\n",
      "56/56 [==============================] - 0s 2ms/step - loss: 8.0372 - accuracy: 0.5230 - val_loss: 15.8663 - val_accuracy: 0.6076\n",
      "Epoch 10/20\n",
      "56/56 [==============================] - 0s 1ms/step - loss: 8.6898 - accuracy: 0.5264 - val_loss: 1.4492 - val_accuracy: 0.4462\n",
      "Epoch 11/20\n",
      "56/56 [==============================] - ETA: 0s - loss: 1.5472 - accuracy: 0.46 - 0s 1ms/step - loss: 5.2808 - accuracy: 0.5241 - val_loss: 7.3216 - val_accuracy: 0.3924\n",
      "Epoch 12/20\n",
      "56/56 [==============================] - 0s 2ms/step - loss: 4.2165 - accuracy: 0.5325 - val_loss: 3.9104 - val_accuracy: 0.3946\n",
      "Epoch 13/20\n",
      "56/56 [==============================] - 0s 2ms/step - loss: 5.2724 - accuracy: 0.5281 - val_loss: 22.6311 - val_accuracy: 0.3901\n",
      "Epoch 14/20\n",
      "56/56 [==============================] - 0s 1ms/step - loss: 8.7813 - accuracy: 0.5236 - val_loss: 11.7517 - val_accuracy: 0.6076\n",
      "Epoch 15/20\n",
      "56/56 [==============================] - 0s 2ms/step - loss: 5.3227 - accuracy: 0.5449 - val_loss: 0.8743 - val_accuracy: 0.5381\n",
      "Epoch 16/20\n",
      "56/56 [==============================] - 0s 1ms/step - loss: 3.0816 - accuracy: 0.5180 - val_loss: 2.5446 - val_accuracy: 0.4058\n",
      "Epoch 17/20\n",
      "56/56 [==============================] - 0s 2ms/step - loss: 2.3624 - accuracy: 0.5309 - val_loss: 2.7812 - val_accuracy: 0.6076\n",
      "Epoch 18/20\n",
      "56/56 [==============================] - 0s 1ms/step - loss: 1.3020 - accuracy: 0.5527 - val_loss: 0.7346 - val_accuracy: 0.6076\n",
      "Epoch 19/20\n",
      "56/56 [==============================] - 0s 1ms/step - loss: 1.1557 - accuracy: 0.5236 - val_loss: 1.6971 - val_accuracy: 0.6054\n",
      "Epoch 20/20\n",
      "56/56 [==============================] - 0s 1ms/step - loss: 1.0951 - accuracy: 0.5337 - val_loss: 0.7444 - val_accuracy: 0.6054\n",
      "Accuracy: 0.5978456014362658\n",
      "Confusion Matrix:\n",
      " [[333   0]\n",
      " [224   0]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      1.00      0.75       333\n",
      "           1       0.00      0.00      0.00       224\n",
      "\n",
      "    accuracy                           0.60       557\n",
      "   macro avg       0.30      0.50      0.37       557\n",
      "weighted avg       0.36      0.60      0.45       557\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\AARON\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\AARON\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\AARON\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "df = pd.read_excel('waterdf1.xlsx')\n",
    "X = df.drop('Potability', axis=1)\n",
    "y = df['Potability']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Input(shape=(X_train.shape[1],)),  # Input layer\n",
    "    tf.keras.layers.Dense(128, activation='relu'),    # Hidden layer with 128 neurons and ReLU activation\n",
    "    tf.keras.layers.Dense(64, activation='relu'),     # Hidden layer with 64 neurons and ReLU activation\n",
    "    tf.keras.layers.Dense(32, activation='relu'),     # Hidden layer with 32 neurons and ReLU activation\n",
    "    tf.keras.layers.Dense(16,activation= tf.keras.layers.LeakyReLU()),     # Hidden layer with 16 neurons and ReLU activation\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')    # Output layer with 1 neuron and sigmoid activation\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model.fit(X_train, y_train, epochs=20, batch_size=32, verbose=1, validation_split=0.2)\n",
    "y_pred_probs = model.predict(X_test)\n",
    "y_pred = (y_pred_probs > 0.5).astype(int)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "classification_rep = classification_report(y_test, y_pred)\n",
    "\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Confusion Matrix:\\n\", conf_matrix)\n",
    "print(\"Classification Report:\\n\", classification_rep)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "56/56 [==============================] - 1s 3ms/step - loss: 0.7559 - accuracy: 0.5152 - val_loss: 0.6867 - val_accuracy: 0.6076\n",
      "Epoch 2/20\n",
      "56/56 [==============================] - 0s 2ms/step - loss: 0.6920 - accuracy: 0.6061 - val_loss: 0.6712 - val_accuracy: 0.6076\n",
      "Epoch 3/20\n",
      "56/56 [==============================] - 0s 1ms/step - loss: 0.6708 - accuracy: 0.6061 - val_loss: 0.6703 - val_accuracy: 0.6076\n",
      "Epoch 4/20\n",
      "56/56 [==============================] - 0s 1ms/step - loss: 0.6711 - accuracy: 0.6061 - val_loss: 0.6711 - val_accuracy: 0.6076\n",
      "Epoch 5/20\n",
      "56/56 [==============================] - 0s 1ms/step - loss: 0.6710 - accuracy: 0.6061 - val_loss: 0.6708 - val_accuracy: 0.6076\n",
      "Epoch 6/20\n",
      "56/56 [==============================] - 0s 1ms/step - loss: 0.6711 - accuracy: 0.6061 - val_loss: 0.6708 - val_accuracy: 0.6076\n",
      "Epoch 7/20\n",
      "56/56 [==============================] - 0s 2ms/step - loss: 0.6709 - accuracy: 0.6061 - val_loss: 0.6708 - val_accuracy: 0.6076\n",
      "Epoch 8/20\n",
      "56/56 [==============================] - 0s 2ms/step - loss: 0.6709 - accuracy: 0.6061 - val_loss: 0.6711 - val_accuracy: 0.6076\n",
      "Epoch 9/20\n",
      "56/56 [==============================] - 0s 1ms/step - loss: 0.6713 - accuracy: 0.6061 - val_loss: 0.6709 - val_accuracy: 0.6076\n",
      "Epoch 10/20\n",
      "56/56 [==============================] - 0s 1ms/step - loss: 0.6710 - accuracy: 0.6061 - val_loss: 0.6709 - val_accuracy: 0.6076\n",
      "Epoch 11/20\n",
      "56/56 [==============================] - 0s 1ms/step - loss: 0.6709 - accuracy: 0.6061 - val_loss: 0.6714 - val_accuracy: 0.6076\n",
      "Epoch 12/20\n",
      "56/56 [==============================] - 0s 1ms/step - loss: 0.6711 - accuracy: 0.6061 - val_loss: 0.6732 - val_accuracy: 0.6076\n",
      "Epoch 13/20\n",
      "56/56 [==============================] - 0s 1ms/step - loss: 0.6708 - accuracy: 0.6061 - val_loss: 0.6709 - val_accuracy: 0.6076\n",
      "Epoch 14/20\n",
      "56/56 [==============================] - 0s 1ms/step - loss: 0.6705 - accuracy: 0.6061 - val_loss: 0.6710 - val_accuracy: 0.6076\n",
      "Epoch 15/20\n",
      "56/56 [==============================] - 0s 1ms/step - loss: 0.6715 - accuracy: 0.6061 - val_loss: 0.6718 - val_accuracy: 0.6076\n",
      "Epoch 16/20\n",
      "56/56 [==============================] - 0s 1ms/step - loss: 0.6711 - accuracy: 0.6061 - val_loss: 0.6708 - val_accuracy: 0.6076\n",
      "Epoch 17/20\n",
      "56/56 [==============================] - 0s 2ms/step - loss: 0.6706 - accuracy: 0.6061 - val_loss: 0.6709 - val_accuracy: 0.6076\n",
      "Epoch 18/20\n",
      "56/56 [==============================] - 0s 1ms/step - loss: 0.6709 - accuracy: 0.6061 - val_loss: 0.6708 - val_accuracy: 0.6076\n",
      "Epoch 19/20\n",
      "56/56 [==============================] - 0s 1ms/step - loss: 0.6710 - accuracy: 0.6061 - val_loss: 0.6711 - val_accuracy: 0.6076\n",
      "Epoch 20/20\n",
      "56/56 [==============================] - 0s 1ms/step - loss: 0.6706 - accuracy: 0.6061 - val_loss: 0.6708 - val_accuracy: 0.6076\n",
      "Accuracy: 0.5978456014362658\n",
      "Confusion Matrix:\n",
      " [[333   0]\n",
      " [224   0]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      1.00      0.75       333\n",
      "           1       0.00      0.00      0.00       224\n",
      "\n",
      "    accuracy                           0.60       557\n",
      "   macro avg       0.30      0.50      0.37       557\n",
      "weighted avg       0.36      0.60      0.45       557\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\AARON\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\AARON\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\AARON\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "df = pd.read_excel('waterdf1.xlsx')\n",
    "X = df.drop('Potability', axis=1)\n",
    "y = df['Potability']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Input(shape=(X_train.shape[1],)),  # Input layer\n",
    "    tf.keras.layers.Dense(128, activation='relu'),    # Hidden layer with 128 neurons and ReLU activation\n",
    "    tf.keras.layers.Dense(64, activation='relu'),     # Hidden layer with 64 neurons and ReLU activation\n",
    "    tf.keras.layers.Dense(32, activation='relu'),     # Hidden layer with 32 neurons and ReLU activation\n",
    "    tf.keras.layers.Dense(36,activation='tanh'),     # Hidden layer with 16 neurons and ReLU activation\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')    # Output layer with 1 neuron and sigmoid activation\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model.fit(X_train, y_train, epochs=20, batch_size=32, verbose=1, validation_split=0.2)\n",
    "y_pred_probs = model.predict(X_test)\n",
    "y_pred = (y_pred_probs > 0.5).astype(int)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "classification_rep = classification_report(y_test, y_pred)\n",
    "\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Confusion Matrix:\\n\", conf_matrix)\n",
    "print(\"Classification Report:\\n\", classification_rep)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2785 entries, 0 to 2784\n",
      "Data columns (total 5 columns):\n",
      " #   Column        Non-Null Count  Dtype  \n",
      "---  ------        --------------  -----  \n",
      " 0   ph            2785 non-null   float64\n",
      " 1   Solids        2785 non-null   float64\n",
      " 2   Conductivity  2785 non-null   float64\n",
      " 3   Turbidity     2785 non-null   float64\n",
      " 4   Potability    2785 non-null   int64  \n",
      "dtypes: float64(4), int64(1)\n",
      "memory usage: 108.9 KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:          B.O.D. (mg/l)   R-squared:                       0.321\n",
      "Model:                            OLS   Adj. R-squared:                  0.320\n",
      "Method:                 Least Squares   F-statistic:                     173.5\n",
      "Date:                Mon, 04 Sep 2023   Prob (F-statistic):          1.08e-121\n",
      "Time:                        19:32:43   Log-Likelihood:                -1371.9\n",
      "No. Observations:                1470   AIC:                             2754.\n",
      "Df Residuals:                    1465   BIC:                             2780.\n",
      "Df Model:                           4                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const          1.2640      0.016     78.632      0.000       1.232       1.296\n",
      "x1            -0.2232      0.016    -13.710      0.000      -0.255      -0.191\n",
      "x2            -0.3426      0.016    -20.948      0.000      -0.375      -0.311\n",
      "x3            -0.0039      0.016     -0.240      0.810      -0.035       0.028\n",
      "x4             0.0669      0.017      4.050      0.000       0.035       0.099\n",
      "==============================================================================\n",
      "Omnibus:                      174.785   Durbin-Watson:                   2.030\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):              263.092\n",
      "Skew:                           0.849   Prob(JB):                     7.42e-58\n",
      "Kurtosis:                       4.188   Cond. No.                         1.27\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "import statsmodels.api as sm  # Import statsmodels library\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('water_dataX1.csv', encoding='latin1')\n",
    "\n",
    "# Select the columns of interest\n",
    "selected_features = ['Temp', 'D.O. (mg/l)', 'PH', 'CONDUCTIVITY (µmhos/cm)', 'B.O.D. (mg/l)']\n",
    "df = df[selected_features]\n",
    "\n",
    "# Convert columns to numeric, handling errors\n",
    "df = df.apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "# Drop rows with NaN values\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "# Remove rows with 'B.O.D. (mg/l)' greater than 300\n",
    "df = df[df['B.O.D. (mg/l)'] <= 100]\n",
    "\n",
    "# Apply a logarithmic transformation to the target variable\n",
    "y = np.log1p(df['B.O.D. (mg/l)'])  # Logarithmic transformation\n",
    "\n",
    "# Define the independent variables (X)\n",
    "X = df[['Temp', 'D.O. (mg/l)', 'PH', 'CONDUCTIVITY (µmhos/cm)']]\n",
    "\n",
    "# Split data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardize the feature values\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Add a constant (intercept) term to the independent variables\n",
    "X_train = sm.add_constant(X_train)\n",
    "X_test = sm.add_constant(X_test)\n",
    "\n",
    "# Create and fit the OLS model\n",
    "model = sm.OLS(y_train, X_train).fit()\n",
    "\n",
    "# Get the summary of the model, including p-values\n",
    "print(model.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "46/46 - 1s - loss: 1.4134 - val_loss: 0.5387 - 1s/epoch - 31ms/step\n",
      "Epoch 2/100\n",
      "46/46 - 0s - loss: 0.4597 - val_loss: 0.3467 - 55ms/epoch - 1ms/step\n",
      "Epoch 3/100\n",
      "46/46 - 0s - loss: 0.3813 - val_loss: 0.3164 - 52ms/epoch - 1ms/step\n",
      "Epoch 4/100\n",
      "46/46 - 0s - loss: 0.3573 - val_loss: 0.3168 - 54ms/epoch - 1ms/step\n",
      "Epoch 5/100\n",
      "46/46 - 0s - loss: 0.3454 - val_loss: 0.3114 - 56ms/epoch - 1ms/step\n",
      "Epoch 6/100\n",
      "46/46 - 0s - loss: 0.3364 - val_loss: 0.3121 - 55ms/epoch - 1ms/step\n",
      "Epoch 7/100\n",
      "46/46 - 0s - loss: 0.3324 - val_loss: 0.3052 - 51ms/epoch - 1ms/step\n",
      "Epoch 8/100\n",
      "46/46 - 0s - loss: 0.3312 - val_loss: 0.3076 - 51ms/epoch - 1ms/step\n",
      "Epoch 9/100\n",
      "46/46 - 0s - loss: 0.3270 - val_loss: 0.3076 - 53ms/epoch - 1ms/step\n",
      "Epoch 10/100\n",
      "46/46 - 0s - loss: 0.3196 - val_loss: 0.3093 - 54ms/epoch - 1ms/step\n",
      "Epoch 11/100\n",
      "46/46 - 0s - loss: 0.3201 - val_loss: 0.3084 - 55ms/epoch - 1ms/step\n",
      "Epoch 12/100\n",
      "46/46 - 0s - loss: 0.3150 - val_loss: 0.3072 - 52ms/epoch - 1ms/step\n",
      "Epoch 13/100\n",
      "46/46 - 0s - loss: 0.3151 - val_loss: 0.3009 - 57ms/epoch - 1ms/step\n",
      "Epoch 14/100\n",
      "46/46 - 0s - loss: 0.3125 - val_loss: 0.3043 - 54ms/epoch - 1ms/step\n",
      "Epoch 15/100\n",
      "46/46 - 0s - loss: 0.3119 - val_loss: 0.2932 - 53ms/epoch - 1ms/step\n",
      "Epoch 16/100\n",
      "46/46 - 0s - loss: 0.3091 - val_loss: 0.2954 - 55ms/epoch - 1ms/step\n",
      "Epoch 17/100\n",
      "46/46 - 0s - loss: 0.3089 - val_loss: 0.3018 - 53ms/epoch - 1ms/step\n",
      "Epoch 18/100\n",
      "46/46 - 0s - loss: 0.3073 - val_loss: 0.2885 - 54ms/epoch - 1ms/step\n",
      "Epoch 19/100\n",
      "46/46 - 0s - loss: 0.3013 - val_loss: 0.2937 - 57ms/epoch - 1ms/step\n",
      "Epoch 20/100\n",
      "46/46 - 0s - loss: 0.3046 - val_loss: 0.2834 - 53ms/epoch - 1ms/step\n",
      "Epoch 21/100\n",
      "46/46 - 0s - loss: 0.2982 - val_loss: 0.2895 - 53ms/epoch - 1ms/step\n",
      "Epoch 22/100\n",
      "46/46 - 0s - loss: 0.2947 - val_loss: 0.2768 - 53ms/epoch - 1ms/step\n",
      "Epoch 23/100\n",
      "46/46 - 0s - loss: 0.2973 - val_loss: 0.2724 - 55ms/epoch - 1ms/step\n",
      "Epoch 24/100\n",
      "46/46 - 0s - loss: 0.2939 - val_loss: 0.2743 - 55ms/epoch - 1ms/step\n",
      "Epoch 25/100\n",
      "46/46 - 0s - loss: 0.2925 - val_loss: 0.2885 - 54ms/epoch - 1ms/step\n",
      "Epoch 26/100\n",
      "46/46 - 0s - loss: 0.2880 - val_loss: 0.2738 - 57ms/epoch - 1ms/step\n",
      "Epoch 27/100\n",
      "46/46 - 0s - loss: 0.2827 - val_loss: 0.2604 - 52ms/epoch - 1ms/step\n",
      "Epoch 28/100\n",
      "46/46 - 0s - loss: 0.2827 - val_loss: 0.2611 - 55ms/epoch - 1ms/step\n",
      "Epoch 29/100\n",
      "46/46 - 0s - loss: 0.2788 - val_loss: 0.2666 - 52ms/epoch - 1ms/step\n",
      "Epoch 30/100\n",
      "46/46 - 0s - loss: 0.2757 - val_loss: 0.2466 - 52ms/epoch - 1ms/step\n",
      "Epoch 31/100\n",
      "46/46 - 0s - loss: 0.2771 - val_loss: 0.2519 - 53ms/epoch - 1ms/step\n",
      "Epoch 32/100\n",
      "46/46 - 0s - loss: 0.2786 - val_loss: 0.2485 - 55ms/epoch - 1ms/step\n",
      "Epoch 33/100\n",
      "46/46 - 0s - loss: 0.2785 - val_loss: 0.2557 - 53ms/epoch - 1ms/step\n",
      "Epoch 34/100\n",
      "46/46 - 0s - loss: 0.2697 - val_loss: 0.2392 - 57ms/epoch - 1ms/step\n",
      "Epoch 35/100\n",
      "46/46 - 0s - loss: 0.2631 - val_loss: 0.2470 - 52ms/epoch - 1ms/step\n",
      "Epoch 36/100\n",
      "46/46 - 0s - loss: 0.2650 - val_loss: 0.2385 - 52ms/epoch - 1ms/step\n",
      "Epoch 37/100\n",
      "46/46 - 0s - loss: 0.2633 - val_loss: 0.2464 - 53ms/epoch - 1ms/step\n",
      "Epoch 38/100\n",
      "46/46 - 0s - loss: 0.2618 - val_loss: 0.2391 - 54ms/epoch - 1ms/step\n",
      "Epoch 39/100\n",
      "46/46 - 0s - loss: 0.2606 - val_loss: 0.2384 - 55ms/epoch - 1ms/step\n",
      "Epoch 40/100\n",
      "46/46 - 0s - loss: 0.2598 - val_loss: 0.2426 - 53ms/epoch - 1ms/step\n",
      "Epoch 41/100\n",
      "46/46 - 0s - loss: 0.2596 - val_loss: 0.2304 - 54ms/epoch - 1ms/step\n",
      "Epoch 42/100\n",
      "46/46 - 0s - loss: 0.2569 - val_loss: 0.2349 - 52ms/epoch - 1ms/step\n",
      "Epoch 43/100\n",
      "46/46 - 0s - loss: 0.2610 - val_loss: 0.2312 - 51ms/epoch - 1ms/step\n",
      "Epoch 44/100\n",
      "46/46 - 0s - loss: 0.2524 - val_loss: 0.2344 - 51ms/epoch - 1ms/step\n",
      "Epoch 45/100\n",
      "46/46 - 0s - loss: 0.2594 - val_loss: 0.2302 - 52ms/epoch - 1ms/step\n",
      "Epoch 46/100\n",
      "46/46 - 0s - loss: 0.2553 - val_loss: 0.2298 - 54ms/epoch - 1ms/step\n",
      "Epoch 47/100\n",
      "46/46 - 0s - loss: 0.2536 - val_loss: 0.2338 - 57ms/epoch - 1ms/step\n",
      "Epoch 48/100\n",
      "46/46 - 0s - loss: 0.2520 - val_loss: 0.2362 - 54ms/epoch - 1ms/step\n",
      "Epoch 49/100\n",
      "46/46 - 0s - loss: 0.2491 - val_loss: 0.2324 - 53ms/epoch - 1ms/step\n",
      "Epoch 50/100\n",
      "46/46 - 0s - loss: 0.2483 - val_loss: 0.2256 - 52ms/epoch - 1ms/step\n",
      "Epoch 51/100\n",
      "46/46 - 0s - loss: 0.2535 - val_loss: 0.2267 - 51ms/epoch - 1ms/step\n",
      "Epoch 52/100\n",
      "46/46 - 0s - loss: 0.2501 - val_loss: 0.2257 - 55ms/epoch - 1ms/step\n",
      "Epoch 53/100\n",
      "46/46 - 0s - loss: 0.2472 - val_loss: 0.2291 - 70ms/epoch - 2ms/step\n",
      "Epoch 54/100\n",
      "46/46 - 0s - loss: 0.2529 - val_loss: 0.2279 - 60ms/epoch - 1ms/step\n",
      "Epoch 55/100\n",
      "46/46 - 0s - loss: 0.2493 - val_loss: 0.2302 - 60ms/epoch - 1ms/step\n",
      "Epoch 56/100\n",
      "46/46 - 0s - loss: 0.2454 - val_loss: 0.2301 - 58ms/epoch - 1ms/step\n",
      "Epoch 57/100\n",
      "46/46 - 0s - loss: 0.2464 - val_loss: 0.2226 - 57ms/epoch - 1ms/step\n",
      "Epoch 58/100\n",
      "46/46 - 0s - loss: 0.2460 - val_loss: 0.2457 - 51ms/epoch - 1ms/step\n",
      "Epoch 59/100\n",
      "46/46 - 0s - loss: 0.2472 - val_loss: 0.2290 - 55ms/epoch - 1ms/step\n",
      "Epoch 60/100\n",
      "46/46 - 0s - loss: 0.2458 - val_loss: 0.2319 - 53ms/epoch - 1ms/step\n",
      "Epoch 61/100\n",
      "46/46 - 0s - loss: 0.2431 - val_loss: 0.2274 - 52ms/epoch - 1ms/step\n",
      "Epoch 62/100\n",
      "46/46 - 0s - loss: 0.2430 - val_loss: 0.2320 - 52ms/epoch - 1ms/step\n",
      "Epoch 63/100\n",
      "46/46 - 0s - loss: 0.2412 - val_loss: 0.2234 - 52ms/epoch - 1ms/step\n",
      "Epoch 64/100\n",
      "46/46 - 0s - loss: 0.2386 - val_loss: 0.2287 - 52ms/epoch - 1ms/step\n",
      "Epoch 65/100\n",
      "46/46 - 0s - loss: 0.2427 - val_loss: 0.2341 - 54ms/epoch - 1ms/step\n",
      "Epoch 66/100\n",
      "46/46 - 0s - loss: 0.2414 - val_loss: 0.2360 - 54ms/epoch - 1ms/step\n",
      "Epoch 67/100\n",
      "46/46 - 0s - loss: 0.2420 - val_loss: 0.2327 - 53ms/epoch - 1ms/step\n",
      "Epoch 68/100\n",
      "46/46 - 0s - loss: 0.2414 - val_loss: 0.2296 - 52ms/epoch - 1ms/step\n",
      "Epoch 69/100\n",
      "46/46 - 0s - loss: 0.2468 - val_loss: 0.2406 - 53ms/epoch - 1ms/step\n",
      "Epoch 70/100\n",
      "46/46 - 0s - loss: 0.2462 - val_loss: 0.2726 - 54ms/epoch - 1ms/step\n",
      "Epoch 71/100\n",
      "46/46 - 0s - loss: 0.2448 - val_loss: 0.2292 - 54ms/epoch - 1ms/step\n",
      "Epoch 72/100\n",
      "46/46 - 0s - loss: 0.2433 - val_loss: 0.2312 - 56ms/epoch - 1ms/step\n",
      "Epoch 73/100\n",
      "46/46 - 0s - loss: 0.2414 - val_loss: 0.2209 - 54ms/epoch - 1ms/step\n",
      "Epoch 74/100\n",
      "46/46 - 0s - loss: 0.2408 - val_loss: 0.2200 - 55ms/epoch - 1ms/step\n",
      "Epoch 75/100\n",
      "46/46 - 0s - loss: 0.2369 - val_loss: 0.2209 - 56ms/epoch - 1ms/step\n",
      "Epoch 76/100\n",
      "46/46 - 0s - loss: 0.2383 - val_loss: 0.2318 - 51ms/epoch - 1ms/step\n",
      "Epoch 77/100\n",
      "46/46 - 0s - loss: 0.2478 - val_loss: 0.2260 - 55ms/epoch - 1ms/step\n",
      "Epoch 78/100\n",
      "46/46 - 0s - loss: 0.2383 - val_loss: 0.2272 - 53ms/epoch - 1ms/step\n",
      "Epoch 79/100\n",
      "46/46 - 0s - loss: 0.2476 - val_loss: 0.2344 - 51ms/epoch - 1ms/step\n",
      "Epoch 80/100\n",
      "46/46 - 0s - loss: 0.2428 - val_loss: 0.2236 - 53ms/epoch - 1ms/step\n",
      "Epoch 81/100\n",
      "46/46 - 0s - loss: 0.2419 - val_loss: 0.2299 - 57ms/epoch - 1ms/step\n",
      "Epoch 82/100\n",
      "46/46 - 0s - loss: 0.2337 - val_loss: 0.2243 - 52ms/epoch - 1ms/step\n",
      "Epoch 83/100\n",
      "46/46 - 0s - loss: 0.2357 - val_loss: 0.2266 - 51ms/epoch - 1ms/step\n",
      "Epoch 84/100\n",
      "46/46 - 0s - loss: 0.2354 - val_loss: 0.2315 - 52ms/epoch - 1ms/step\n",
      "Epoch 85/100\n",
      "46/46 - 0s - loss: 0.2404 - val_loss: 0.2344 - 53ms/epoch - 1ms/step\n",
      "Epoch 86/100\n",
      "46/46 - 0s - loss: 0.2405 - val_loss: 0.2265 - 59ms/epoch - 1ms/step\n",
      "Epoch 87/100\n",
      "46/46 - 0s - loss: 0.2360 - val_loss: 0.2252 - 55ms/epoch - 1ms/step\n",
      "Epoch 88/100\n",
      "46/46 - 0s - loss: 0.2350 - val_loss: 0.2246 - 54ms/epoch - 1ms/step\n",
      "Epoch 89/100\n",
      "46/46 - 0s - loss: 0.2328 - val_loss: 0.2297 - 54ms/epoch - 1ms/step\n",
      "Epoch 90/100\n",
      "46/46 - 0s - loss: 0.2349 - val_loss: 0.2245 - 54ms/epoch - 1ms/step\n",
      "Epoch 91/100\n",
      "46/46 - 0s - loss: 0.2336 - val_loss: 0.2287 - 54ms/epoch - 1ms/step\n",
      "Epoch 92/100\n",
      "46/46 - 0s - loss: 0.2352 - val_loss: 0.2448 - 52ms/epoch - 1ms/step\n",
      "Epoch 93/100\n",
      "46/46 - 0s - loss: 0.2329 - val_loss: 0.2209 - 55ms/epoch - 1ms/step\n",
      "Epoch 94/100\n",
      "46/46 - 0s - loss: 0.2315 - val_loss: 0.2277 - 53ms/epoch - 1ms/step\n",
      "Epoch 95/100\n",
      "46/46 - 0s - loss: 0.2303 - val_loss: 0.2260 - 53ms/epoch - 1ms/step\n",
      "Epoch 96/100\n",
      "46/46 - 0s - loss: 0.2341 - val_loss: 0.2299 - 53ms/epoch - 1ms/step\n",
      "Epoch 97/100\n",
      "46/46 - 0s - loss: 0.2366 - val_loss: 0.2255 - 53ms/epoch - 1ms/step\n",
      "Epoch 98/100\n",
      "46/46 - 0s - loss: 0.2351 - val_loss: 0.2190 - 52ms/epoch - 1ms/step\n",
      "Epoch 99/100\n",
      "46/46 - 0s - loss: 0.2317 - val_loss: 0.2218 - 52ms/epoch - 1ms/step\n",
      "Epoch 100/100\n",
      "46/46 - 0s - loss: 0.2294 - val_loss: 0.2221 - 56ms/epoch - 1ms/step\n",
      "12/12 [==============================] - 0s 778us/step - loss: 0.2221\n",
      "Mean Squared Error on Test Set (with log-transformed target): 0.2221142053604126\n",
      "Mean Squared Error on Test Set (original scale): 20.059642299897014\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('water_dataX1.csv', encoding='latin1')\n",
    "\n",
    "# Select the columns of interest\n",
    "selected_features = ['Temp', 'D.O. (mg/l)','CONDUCTIVITY (µmhos/cm)', 'B.O.D. (mg/l)']\n",
    "df = df[selected_features]\n",
    "\n",
    "# Convert columns to numeric, handling errors\n",
    "df = df.apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "# Drop rows with NaN values\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "# Remove rows with 'B.O.D. (mg/l)' greater than 300\n",
    "df = df[df['B.O.D. (mg/l)'] <= 100]\n",
    "\n",
    "# Split the data into features (X) and target variable (y)\n",
    "X = df[['Temp', 'D.O. (mg/l)','CONDUCTIVITY (µmhos/cm)']]\n",
    "y = df['B.O.D. (mg/l)']\n",
    "\n",
    "# Apply a logarithmic transformation to the target variable\n",
    "y = np.log1p(y)  # Logarithmic transformation\n",
    "\n",
    "# Split data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardize the feature values\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Build the neural network model\n",
    "model = Sequential()\n",
    "model.add(Dense(64, input_dim=X_train.shape[1], activation='relu'))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(16, activation='relu'))  # Change activation to 'relu'\n",
    "\n",
    "# Output layer with linear activation for regression\n",
    "model.add(Dense(1, activation='linear'))  # Change activation to 'linear'\n",
    "\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train, y_train, epochs=100, batch_size=32, validation_data=(X_test, y_test), verbose=2)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "loss = model.evaluate(X_test, y_test)\n",
    "print(\"Mean Squared Error on Test Set (with log-transformed target):\", loss)\n",
    "\n",
    "# Inverse log transformation for predictions and true values\n",
    "y_pred_original = np.expm1(y_pred).flatten()\n",
    "y_test_original = np.expm1(y_test).to_numpy().flatten()\n",
    "\n",
    "# Calculate the mean squared error on the original scale.\n",
    "mse_original_scale = np.mean(np.square(y_pred_original - y_test_original))\n",
    "print(\"Mean Squared Error on Test Set (original scale):\", mse_original_scale)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "56/56 [==============================] - 1s 4ms/step - loss: 0.7133 - accuracy: 0.5095 - val_loss: 0.7056 - val_accuracy: 0.6076\n",
      "Epoch 2/20\n",
      "56/56 [==============================] - 0s 2ms/step - loss: 0.6784 - accuracy: 0.6061 - val_loss: 0.6698 - val_accuracy: 0.6076\n",
      "Epoch 3/20\n",
      "56/56 [==============================] - 0s 2ms/step - loss: 0.6715 - accuracy: 0.6061 - val_loss: 0.6705 - val_accuracy: 0.6076\n",
      "Epoch 4/20\n",
      "56/56 [==============================] - 0s 2ms/step - loss: 0.6717 - accuracy: 0.6061 - val_loss: 0.6698 - val_accuracy: 0.6076\n",
      "Epoch 5/20\n",
      "56/56 [==============================] - 0s 2ms/step - loss: 0.6710 - accuracy: 0.6061 - val_loss: 0.6697 - val_accuracy: 0.6076\n",
      "Epoch 6/20\n",
      "56/56 [==============================] - 0s 2ms/step - loss: 0.6714 - accuracy: 0.6061 - val_loss: 0.6697 - val_accuracy: 0.6076\n",
      "Epoch 7/20\n",
      "56/56 [==============================] - 0s 2ms/step - loss: 0.6713 - accuracy: 0.6061 - val_loss: 0.6709 - val_accuracy: 0.6076\n",
      "Epoch 8/20\n",
      "56/56 [==============================] - 0s 2ms/step - loss: 0.6714 - accuracy: 0.6061 - val_loss: 0.6698 - val_accuracy: 0.6076\n",
      "Epoch 9/20\n",
      "56/56 [==============================] - 0s 2ms/step - loss: 0.6716 - accuracy: 0.6061 - val_loss: 0.6711 - val_accuracy: 0.6076\n",
      "Epoch 10/20\n",
      "56/56 [==============================] - 0s 2ms/step - loss: 0.6720 - accuracy: 0.6061 - val_loss: 0.6697 - val_accuracy: 0.6076\n",
      "Epoch 11/20\n",
      "56/56 [==============================] - 0s 2ms/step - loss: 0.6713 - accuracy: 0.6061 - val_loss: 0.6703 - val_accuracy: 0.6076\n",
      "Epoch 12/20\n",
      "56/56 [==============================] - 0s 2ms/step - loss: 0.6715 - accuracy: 0.6061 - val_loss: 0.6698 - val_accuracy: 0.6076\n",
      "Epoch 13/20\n",
      "56/56 [==============================] - 0s 2ms/step - loss: 0.6714 - accuracy: 0.6061 - val_loss: 0.6697 - val_accuracy: 0.6076\n",
      "Epoch 14/20\n",
      "56/56 [==============================] - 0s 2ms/step - loss: 0.6717 - accuracy: 0.6061 - val_loss: 0.6699 - val_accuracy: 0.6076\n",
      "Epoch 15/20\n",
      "56/56 [==============================] - 0s 1ms/step - loss: 0.6724 - accuracy: 0.6061 - val_loss: 0.6701 - val_accuracy: 0.6076\n",
      "Epoch 16/20\n",
      "56/56 [==============================] - 0s 2ms/step - loss: 0.6716 - accuracy: 0.6061 - val_loss: 0.6697 - val_accuracy: 0.6076\n",
      "Epoch 17/20\n",
      "56/56 [==============================] - 0s 2ms/step - loss: 0.6707 - accuracy: 0.6061 - val_loss: 0.6706 - val_accuracy: 0.6076\n",
      "Epoch 18/20\n",
      "56/56 [==============================] - 0s 2ms/step - loss: 0.6717 - accuracy: 0.6061 - val_loss: 0.6713 - val_accuracy: 0.6076\n",
      "Epoch 19/20\n",
      "56/56 [==============================] - 0s 2ms/step - loss: 0.6718 - accuracy: 0.6061 - val_loss: 0.6698 - val_accuracy: 0.6076\n",
      "Epoch 20/20\n",
      "56/56 [==============================] - 0s 2ms/step - loss: 0.6710 - accuracy: 0.6061 - val_loss: 0.6699 - val_accuracy: 0.6076\n",
      "Accuracy: 0.5978456014362658\n",
      "Confusion Matrix:\n",
      " [[333   0]\n",
      " [224   0]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      1.00      0.75       333\n",
      "           1       0.00      0.00      0.00       224\n",
      "\n",
      "    accuracy                           0.60       557\n",
      "   macro avg       0.30      0.50      0.37       557\n",
      "weighted avg       0.36      0.60      0.45       557\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\AARON\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\AARON\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\AARON\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "df = pd.read_excel('waterdf1.xlsx')\n",
    "X = df.drop('Potability', axis=1)\n",
    "y = df['Potability']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Input(shape=(X_train.shape[1],)),  # Input layer\n",
    "    tf.keras.layers.Dense(128, activation='relu'),    # Hidden layer with 128 neurons and ReLU activation\n",
    "    tf.keras.layers.Dense(64, activation='relu'),     # Hidden layer with 64 neurons and ReLU activation\n",
    "    tf.keras.layers.Dense(32, activation='relu'),     # Hidden layer with 32 neurons and ReLU activation\n",
    "    tf.keras.layers.Dense(36,activation='tanh'),     # Hidden layer with 16 neurons and ReLU activation\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')    # Output layer with 1 neuron and sigmoid activation\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model.fit(X_train, y_train, epochs=20, batch_size=32, verbose=1, validation_split=0.2)\n",
    "y_pred_probs = model.predict(X_test)\n",
    "y_pred = (y_pred_probs > 0.5).astype(int)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "classification_rep = classification_report(y_test, y_pred)\n",
    "\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Confusion Matrix:\\n\", conf_matrix)\n",
    "print(\"Classification Report:\\n\", classification_rep)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: Predicted Non-Potable\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# Load the trained model # Replace 'your_model_path' with the actual path to your saved model\n",
    "\n",
    "def predict_potability(pH, TDS, conductivity, turbidity):\n",
    "    # Create a DataFrame with the input values\n",
    "    input_data = pd.DataFrame({\n",
    "        'pH': [pH],\n",
    "        'TDS': [TDS],\n",
    "        'Conductivity': [conductivity],\n",
    "        'Turbidity': [turbidity]\n",
    "    })\n",
    "\n",
    "    # Make predictions using the loaded model\n",
    "    y_pred_probs = model.predict(input_data)\n",
    "    y_pred = (y_pred_probs > 0.5).astype(int)\n",
    "\n",
    "    # Interpret the prediction\n",
    "    if y_pred[0][0] == 1:\n",
    "        return \"Predicted Potable\"\n",
    "    else:\n",
    "        return \"Predicted Non-Potable\"\n",
    "\n",
    "# Example usage:\n",
    "pH_value = 7.0\n",
    "TDS_value = 17000\n",
    "conductivity_value = 500\n",
    "turbidity_value = 4\n",
    "\n",
    "prediction = predict_potability(pH_value, TDS_value, conductivity_value, turbidity_value)\n",
    "print(\"Prediction:\", prediction)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting lime\n",
      "  Downloading lime-0.2.0.1.tar.gz (275 kB)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\aaron\\anaconda3\\lib\\site-packages (from lime) (3.5.3)\n",
      "Requirement already satisfied: numpy in c:\\users\\aaron\\anaconda3\\lib\\site-packages (from lime) (1.21.6)\n",
      "Requirement already satisfied: scipy in c:\\users\\aaron\\anaconda3\\lib\\site-packages (from lime) (1.7.3)\n",
      "Requirement already satisfied: tqdm in c:\\users\\aaron\\anaconda3\\lib\\site-packages (from lime) (4.64.0)\n",
      "Requirement already satisfied: scikit-learn>=0.18 in c:\\users\\aaron\\anaconda3\\lib\\site-packages (from lime) (1.2.2)\n",
      "Requirement already satisfied: scikit-image>=0.12 in c:\\users\\aaron\\anaconda3\\lib\\site-packages (from lime) (0.19.2)\n",
      "Requirement already satisfied: PyWavelets>=1.1.1 in c:\\users\\aaron\\anaconda3\\lib\\site-packages (from scikit-image>=0.12->lime) (1.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.Requirement already satisfied: packaging>=20.0 in c:\\users\\aaron\\anaconda3\\lib\\site-packages (from scikit-image>=0.12->lime) (23.0)\n",
      "\n",
      "Requirement already satisfied: networkx>=2.2 in c:\\users\\aaron\\anaconda3\\lib\\site-packages (from scikit-image>=0.12->lime) (2.6.3)\n",
      "Requirement already satisfied: tifffile>=2019.7.26 in c:\\users\\aaron\\anaconda3\\lib\\site-packages (from scikit-image>=0.12->lime) (2021.7.2)\n",
      "Requirement already satisfied: pillow!=7.1.0,!=7.1.1,!=8.3.0,>=6.1.0 in c:\\users\\aaron\\anaconda3\\lib\\site-packages (from scikit-image>=0.12->lime) (9.0.1)\n",
      "Requirement already satisfied: imageio>=2.4.1 in c:\\users\\aaron\\anaconda3\\lib\\site-packages (from scikit-image>=0.12->lime) (2.9.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\aaron\\anaconda3\\lib\\site-packages (from scikit-learn>=0.18->lime) (2.2.0)\n",
      "Requirement already satisfied: joblib>=1.1.1 in c:\\users\\aaron\\anaconda3\\lib\\site-packages (from scikit-learn>=0.18->lime) (1.2.0)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in c:\\users\\aaron\\anaconda3\\lib\\site-packages (from matplotlib->lime) (3.0.4)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\aaron\\anaconda3\\lib\\site-packages (from matplotlib->lime) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\aaron\\anaconda3\\lib\\site-packages (from matplotlib->lime) (4.25.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\aaron\\anaconda3\\lib\\site-packages (from matplotlib->lime) (1.3.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\aaron\\anaconda3\\lib\\site-packages (from matplotlib->lime) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\aaron\\anaconda3\\lib\\site-packages (from python-dateutil>=2.7->matplotlib->lime) (1.16.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\aaron\\appdata\\roaming\\python\\python39\\site-packages (from tqdm->lime) (0.4.5)\n",
      "Building wheels for collected packages: lime\n",
      "  Building wheel for lime (setup.py): started\n",
      "  Building wheel for lime (setup.py): finished with status 'done'\n",
      "  Created wheel for lime: filename=lime-0.2.0.1-py3-none-any.whl size=283839 sha256=1f8028a76f74b878f51886ea16a1ea630ae4733bd975d4683fbf9eb9fd6e28e4\n",
      "  Stored in directory: c:\\users\\aaron\\appdata\\local\\pip\\cache\\wheels\\ed\\d7\\c9\\5a0130d06d6310bc6cbe55220e6e72dcb8c4eff9a478717066\n",
      "Successfully built lime\n",
      "Installing collected packages: lime\n",
      "Successfully installed lime-0.2.0.1\n"
     ]
    }
   ],
   "source": [
    "pip install lime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lime.lime_tabular import LimeTabularExplainer\n",
    "\n",
    "explainer = LimeTabularExplainer(X_train.values, feature_names =\n",
    "                                 list(X_train.columns),verbose=True,\n",
    "                                 class_names = [\"Not Potable\",\"Potable\"],\n",
    "                                 mode = 'classification')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "56/56 [==============================] - 1s 4ms/step - loss: 0.6831 - accuracy: 0.5769 - val_loss: 0.6733 - val_accuracy: 0.6076\n",
      "Epoch 2/20\n",
      "56/56 [==============================] - 0s 1ms/step - loss: 0.6715 - accuracy: 0.6061 - val_loss: 0.6698 - val_accuracy: 0.6076\n",
      "Epoch 3/20\n",
      "56/56 [==============================] - 0s 2ms/step - loss: 0.6708 - accuracy: 0.6061 - val_loss: 0.6706 - val_accuracy: 0.6076\n",
      "Epoch 4/20\n",
      "56/56 [==============================] - 0s 2ms/step - loss: 0.6708 - accuracy: 0.6061 - val_loss: 0.6705 - val_accuracy: 0.6076\n",
      "Epoch 5/20\n",
      "56/56 [==============================] - 0s 2ms/step - loss: 0.6706 - accuracy: 0.6061 - val_loss: 0.6720 - val_accuracy: 0.6076\n",
      "Epoch 6/20\n",
      "56/56 [==============================] - 0s 2ms/step - loss: 0.6716 - accuracy: 0.6061 - val_loss: 0.6715 - val_accuracy: 0.6076\n",
      "Epoch 7/20\n",
      "56/56 [==============================] - 0s 2ms/step - loss: 0.6710 - accuracy: 0.6061 - val_loss: 0.6713 - val_accuracy: 0.6076\n",
      "Epoch 8/20\n",
      "56/56 [==============================] - 0s 2ms/step - loss: 0.6712 - accuracy: 0.6061 - val_loss: 0.6931 - val_accuracy: 0.6076\n",
      "Epoch 9/20\n",
      "56/56 [==============================] - 0s 1ms/step - loss: 0.6773 - accuracy: 0.6061 - val_loss: 0.6702 - val_accuracy: 0.6076\n",
      "Epoch 10/20\n",
      "56/56 [==============================] - 0s 2ms/step - loss: 0.6706 - accuracy: 0.6061 - val_loss: 0.6724 - val_accuracy: 0.6076\n",
      "Epoch 11/20\n",
      "56/56 [==============================] - 0s 1ms/step - loss: 0.6715 - accuracy: 0.6061 - val_loss: 0.6703 - val_accuracy: 0.6076\n",
      "Epoch 12/20\n",
      "56/56 [==============================] - 0s 1ms/step - loss: 0.6714 - accuracy: 0.6061 - val_loss: 0.6706 - val_accuracy: 0.6076\n",
      "Epoch 13/20\n",
      "56/56 [==============================] - 0s 1ms/step - loss: 0.6707 - accuracy: 0.6061 - val_loss: 0.6705 - val_accuracy: 0.6076\n",
      "Epoch 14/20\n",
      "56/56 [==============================] - 0s 2ms/step - loss: 0.6710 - accuracy: 0.6061 - val_loss: 0.6707 - val_accuracy: 0.6076\n",
      "Epoch 15/20\n",
      "56/56 [==============================] - 0s 1ms/step - loss: 0.6709 - accuracy: 0.6061 - val_loss: 0.6724 - val_accuracy: 0.6076\n",
      "Epoch 16/20\n",
      "56/56 [==============================] - 0s 2ms/step - loss: 0.6707 - accuracy: 0.6061 - val_loss: 0.6721 - val_accuracy: 0.6076\n",
      "Epoch 17/20\n",
      "56/56 [==============================] - 0s 2ms/step - loss: 0.6728 - accuracy: 0.6061 - val_loss: 0.6709 - val_accuracy: 0.6076\n",
      "Epoch 18/20\n",
      "56/56 [==============================] - 0s 2ms/step - loss: 0.6715 - accuracy: 0.6061 - val_loss: 0.6709 - val_accuracy: 0.6076\n",
      "Epoch 19/20\n",
      "56/56 [==============================] - 0s 2ms/step - loss: 0.6712 - accuracy: 0.6061 - val_loss: 0.6716 - val_accuracy: 0.6076\n",
      "Epoch 20/20\n",
      "56/56 [==============================] - 0s 2ms/step - loss: 0.6715 - accuracy: 0.6061 - val_loss: 0.6704 - val_accuracy: 0.6076\n",
      "Accuracy: 0.5978456014362658\n",
      "Confusion Matrix:\n",
      " [[333   0]\n",
      " [224   0]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      1.00      0.75       333\n",
      "           1       0.00      0.00      0.00       224\n",
      "\n",
      "    accuracy                           0.60       557\n",
      "   macro avg       0.30      0.50      0.37       557\n",
      "weighted avg       0.36      0.60      0.45       557\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\AARON\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\AARON\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\AARON\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\AARON\\anaconda3\\lib\\site-packages\\lime\\lime_tabular.py:372: UserWarning: \n",
      "                    Prediction probabilties do not sum to 1, and\n",
      "                    thus does not constitute a probability space.\n",
      "                    Check that you classifier outputs probabilities\n",
      "                    (Not log probabilities, or actual class predictions).\n",
      "                    \n",
      "  warnings.warn(\"\"\"\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 1 is out of bounds for axis 1 with size 1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [124]\u001b[0m, in \u001b[0;36m<cell line: 50>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     47\u001b[0m instance_to_explain \u001b[38;5;241m=\u001b[39m X_test\u001b[38;5;241m.\u001b[39mvalues[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m     49\u001b[0m \u001b[38;5;66;03m# Explain the prediction made by your TensorFlow model\u001b[39;00m\n\u001b[1;32m---> 50\u001b[0m explanation \u001b[38;5;241m=\u001b[39m \u001b[43mexplainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexplain_instance\u001b[49m\u001b[43m(\u001b[49m\u001b[43minstance_to_explain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpredict_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpredict_fn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;66;03m# Print the explanation\u001b[39;00m\n\u001b[0;32m     53\u001b[0m explanation\u001b[38;5;241m.\u001b[39mshow_in_notebook()\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\lime\\lime_tabular.py:452\u001b[0m, in \u001b[0;36mLimeTabularExplainer.explain_instance\u001b[1;34m(self, data_row, predict_fn, labels, top_labels, num_features, num_samples, distance_metric, model_regressor)\u001b[0m\n\u001b[0;32m    448\u001b[0m     labels \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    449\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m label \u001b[38;5;129;01min\u001b[39;00m labels:\n\u001b[0;32m    450\u001b[0m     (ret_exp\u001b[38;5;241m.\u001b[39mintercept[label],\n\u001b[0;32m    451\u001b[0m      ret_exp\u001b[38;5;241m.\u001b[39mlocal_exp[label],\n\u001b[1;32m--> 452\u001b[0m      ret_exp\u001b[38;5;241m.\u001b[39mscore, ret_exp\u001b[38;5;241m.\u001b[39mlocal_pred) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbase\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexplain_instance_with_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    453\u001b[0m \u001b[43m            \u001b[49m\u001b[43mscaled_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    454\u001b[0m \u001b[43m            \u001b[49m\u001b[43myss\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    455\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdistances\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    456\u001b[0m \u001b[43m            \u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    457\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnum_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    458\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmodel_regressor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_regressor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    459\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfeature_selection\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeature_selection\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    461\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mregression\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    462\u001b[0m     ret_exp\u001b[38;5;241m.\u001b[39mintercept[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m=\u001b[39m ret_exp\u001b[38;5;241m.\u001b[39mintercept[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\lime\\lime_base.py:182\u001b[0m, in \u001b[0;36mLimeBase.explain_instance_with_data\u001b[1;34m(self, neighborhood_data, neighborhood_labels, distances, label, num_features, feature_selection, model_regressor)\u001b[0m\n\u001b[0;32m    145\u001b[0m \u001b[38;5;124;03m\"\"\"Takes perturbed data, labels and distances, returns explanation.\u001b[39;00m\n\u001b[0;32m    146\u001b[0m \n\u001b[0;32m    147\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    178\u001b[0m \u001b[38;5;124;03m    local_pred is the prediction of the explanation model on the original instance\u001b[39;00m\n\u001b[0;32m    179\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    181\u001b[0m weights \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkernel_fn(distances)\n\u001b[1;32m--> 182\u001b[0m labels_column \u001b[38;5;241m=\u001b[39m \u001b[43mneighborhood_labels\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m    183\u001b[0m used_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeature_selection(neighborhood_data,\n\u001b[0;32m    184\u001b[0m                                        labels_column,\n\u001b[0;32m    185\u001b[0m                                        weights,\n\u001b[0;32m    186\u001b[0m                                        num_features,\n\u001b[0;32m    187\u001b[0m                                        feature_selection)\n\u001b[0;32m    188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m model_regressor \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mIndexError\u001b[0m: index 1 is out of bounds for axis 1 with size 1"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "from lime.lime_tabular import LimeTabularExplainer\n",
    "\n",
    "# Load your data\n",
    "df = pd.read_excel('waterdf1.xlsx')\n",
    "\n",
    "# Split the data into features (X) and target (y)\n",
    "X = df.drop('Potability', axis=1)\n",
    "y = df['Potability']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Input(shape=(X_train.shape[1],)),  # Input layer\n",
    "    tf.keras.layers.Dense(128, activation='relu'),    # Hidden layer with 128 neurons and ReLU activation\n",
    "    tf.keras.layers.Dense(64, activation='relu'),     # Hidden layer with 64 neurons and ReLU activation\n",
    "    tf.keras.layers.Dense(32, activation='relu'),     # Hidden layer with 32 neurons and ReLU activation\n",
    "    tf.keras.layers.Dense(36,activation='tanh'),     # Hidden layer with 16 neurons and ReLU activation\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')    # Output layer with 1 neuron and sigmoid activation\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model.fit(X_train, y_train, epochs=20, batch_size=32, verbose=1, validation_split=0.2)\n",
    "y_pred_probs = model.predict(X_test)\n",
    "y_pred = (y_pred_probs > 0.5).astype(int)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "classification_rep = classification_report(y_test, y_pred)\n",
    "\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Confusion Matrix:\\n\", conf_matrix)\n",
    "print(\"Classification Report:\\n\", classification_rep)\n",
    "# Create a LimeTabularExplainer\n",
    "explainer = LimeTabularExplainer(X_train.values, mode=\"classification\", training_labels=y_train,\n",
    "                                 feature_names=list(X_train.columns))\n",
    "\n",
    "# Define a function to predict with your TensorFlow model\n",
    "def predict_fn(X):\n",
    "    y_pred_probs = model.predict(X)\n",
    "    return y_pred_probs\n",
    "\n",
    "# Choose an instance from your test data to explain (you can change the index)\n",
    "instance_to_explain = X_test.values[0]\n",
    "\n",
    "# Explain the prediction made by your TensorFlow model\n",
    "explanation = explainer.explain_instance(instance_to_explain, predict_fn=predict_fn)\n",
    "\n",
    "# Print the explanation\n",
    "explanation.show_in_notebook()\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
